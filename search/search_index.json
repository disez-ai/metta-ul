{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"metta-ul: Clustering Algorithms in MeTTa","text":""},{"location":"#overview","title":"Overview","text":"<p>metta-ul is a basic implementation of clustering algorithms in the MeTTa language. It includes implementations of:</p> <ul> <li>K-Means</li> <li>Gaussian Mixture Models (GMM)</li> <li>Spectral Clustering</li> <li>Hierarchical Clustering</li> </ul> <p>This project is packaged as a Python module and includes a Dockerized environment for running tests using <code>pytest</code>.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Ramin Barati - rekino@gmail.com</li> <li>Amirhossein Nourani Zadeh - amirhossein.nouranizadeh@gmail.com</li> <li>Farhoud - farhoud.m7@gmail.com</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.7 or later</li> <li>Docker</li> <li><code>hyperon</code> &gt;= 0.2.2</li> <li><code>scikit-learn</code></li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"#using-docker","title":"Using Docker","text":"<p>Build and run the containerized environment:</p> <pre><code>docker build . -t metta_ul\n</code></pre>"},{"location":"#running-tests","title":"Running Tests","text":""},{"location":"#running-tests-inside-docker","title":"Running tests inside Docker","text":"<p>You can run tests using the provided <code>Makefile</code>. This will:</p> <ol> <li>Build the Docker image</li> <li>Run tests inside a container</li> <li>Clean up the container after the test run</li> </ol> <p>To execute:</p> <pre><code>make test\n</code></pre> <p>Alternatively, if you want to run pytest directly inside Docker:</p> <pre><code>docker run -it --rm --mount type=bind,src=$(pwd),dst=/app --name metta_ul_run metta_ul pytest -s\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a new branch (<code>feature-branch</code>)</li> <li>Commit changes and push to your branch</li> <li>Submit a pull request</li> </ol>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"agglomerative-api-reference/","title":"metta_ul:cluster:agglomerative","text":""},{"location":"agglomerative-api-reference/#overview","title":"Overview","text":"<p>This module implements agglomerative clustering using hierarchical merging. The algorithm starts with each data point as its own cluster and iteratively merges the closest clusters based on a chosen linkage criterion.</p>"},{"location":"agglomerative-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"agglomerative-api-reference/#agglomerativeinit-clusters","title":"<code>agglomerative.init-clusters</code>","text":"<p>Initializes clusters where each data point starts as its own cluster.</p>"},{"location":"agglomerative-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$n</code>: The number of data points (clusters initially).<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns","title":"Returns:","text":"<ul> <li>A list where each data point is its own cluster.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativedistance-matrix","title":"<code>agglomerative.distance-matrix</code>","text":"<p>Computes the pairwise Euclidean distance matrix for the dataset.</p>"},{"location":"agglomerative-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A square matrix where the entry at (i, j) represents the Euclidean distance between points i and j.<ul> <li>Type: <code>(NPArray ($n $n))</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativelinkage-distance","title":"<code>agglomerative.linkage-distance</code>","text":"<p>Computes the distance between two clusters based on the specified linkage criterion.</p>"},{"location":"agglomerative-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$distance-matrix</code>: The precomputed distance matrix.<ul> <li>Type: <code>(NPArray ($n $n))</code></li> </ul> </li> <li><code>$cluster1</code>: The first cluster.<ul> <li>Type: <code>PyList</code></li> </ul> </li> <li><code>$cluster2</code>: The second cluster.<ul> <li>Type: <code>PyList</code></li> </ul> </li> <li><code>$linkage</code>: The linkage method to use (<code>\"single\"</code>, <code>\"complete\"</code>, or <code>\"average\"</code>).<ul> <li>Type: <code>String</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>The computed distance between the two clusters.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativeclosest-clusters","title":"<code>agglomerative.closest-clusters</code>","text":"<p>Finds the two clusters that are closest based on the specified linkage criterion.</p>"},{"location":"agglomerative-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$clusters</code>: The list of current clusters.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> <li><code>$distance-matrix</code>: The precomputed distance matrix.<ul> <li>Type: <code>(NPArray ($n $n))</code></li> </ul> </li> <li><code>$linkage</code>: The linkage method to use.<ul> <li>Type: <code>String</code></li> </ul> </li> <li><code>$min-distance</code>: The current minimum distance found.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$closest-pair</code>: The closest pair of clusters found so far.<ul> <li>Type: <code>(PyList PyList)</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>A tuple containing the closest pair of clusters.</li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativemerge-clusters","title":"<code>agglomerative.merge-clusters</code>","text":"<p>Merges the two clusters that are closest based on the specified linkage criterion.</p>"},{"location":"agglomerative-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$clusters</code>: The list of current clusters.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> <li><code>$distance-matrix</code>: The precomputed distance matrix.<ul> <li>Type: <code>(NPArray ($n $n))</code></li> </ul> </li> <li><code>$linkage</code>: The linkage method to use.<ul> <li>Type: <code>String</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>The updated clusters.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativerecursion","title":"<code>agglomerative.recursion</code>","text":"<p>The basic recursion step of the algorithm.</p>"},{"location":"agglomerative-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li><code>$linkage</code>: The linkage method to use (<code>\"single\"</code>, <code>\"complete\"</code>, or <code>\"average\"</code>).<ul> <li>Type: <code>String</code></li> </ul> </li> <li><code>$clusters</code>: The list of current clusters.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> <li><code>$distance-matrix</code>: The precomputed distance matrix.<ul> <li>Type: <code>(NPArray ($n $n))</code></li> </ul> </li> <li><code>$length</code>: The count of clusters in <code>$clusters</code>.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_5","title":"Returns:","text":"<ul> <li>A clusters as a MeTTa list.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerative","title":"<code>agglomerative</code>","text":"<p>Performs agglomerative clustering on a dataset.</p>"},{"location":"agglomerative-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$linkage</code>: The linkage method to use (<code>\"single\"</code>, <code>\"complete\"</code>, or <code>\"average\"</code>).<ul> <li>Type: <code>String</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_6","title":"Returns:","text":"<ul> <li>Clusters as a MeTTa list of Python lists of numbers which represent a row in <code>$X</code>.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativeassign","title":"<code>agglomerative.assign</code>","text":"<p>Transform a MeTTa list of clusters into a Numpy array of assignments.</p>"},{"location":"agglomerative-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li><code>$clusters</code>: The list of clusters.<ul> <li>Type: <code>(List PyList)</code></li> </ul> </li> <li><code>$assignment</code>: The current assignment.<ul> <li>Type: <code>(NPArray ($n))</code></li> </ul> </li> <li><code>$index</code>: The index of the cluster in <code>$clusters</code>.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_7","title":"Returns:","text":"<ul> <li>Assignments as a numpy array.<ul> <li>Type: <code>(NPArray ($n))</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativefit-predict","title":"<code>agglomerative.fit-predict</code>","text":"<p>Clusters a numpy array of samples. (: agglomerative.fit-predict (-&gt; (NPArray ($n $d)) Number String (NPArray ($n)))) (=     (agglomerative.fit-predict $X $k $linkage)</p>"},{"location":"agglomerative-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li><code>$X</code>: The list of clusters.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$k</code>: The number of clusters.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$linkage</code>: The linkage method to use (<code>\"single\"</code>, <code>\"complete\"</code>, or <code>\"average\"</code>).<ul> <li>Type: <code>String</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_8","title":"Returns:","text":"<ul> <li>Assignments as a numpy array.<ul> <li>Type: <code>(NPArray ($n))</code></li> </ul> </li> </ul>"},{"location":"agglomerative-api-reference/#usage","title":"Usage","text":"<p>To cluster a dataset <code>S</code> of type <code>(NPArray ($n $d))</code> with <code>\"average\"</code> linking: <pre><code>(=\n    (assignments)\n    (agglomerative.fit-predict (S) \"average\")\n)\n</code></pre></p>"},{"location":"agglomerative-api-reference/#notes","title":"Notes","text":"<ul> <li>The distance metric used is the Euclidean norm.</li> <li>The algorithm supports different linkage strategies to control the merging behavior.</li> </ul>"},{"location":"agglomerative-technical/","title":"Agglomerative Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 21, 2025 Version: 1.0  </p>"},{"location":"agglomerative-technical/#abstract","title":"Abstract","text":"<p>This report presents an open\u2011source implementation of agglomerative (hierarchical) clustering in the MeTTa language. By leveraging MeTTa\u2019s declarative recursion and linked\u2010list constructs, it performs bottom\u2010up cluster merging with selectable linkage criteria (single, complete, average). We analyze time and memory complexities at each step and compare design features to scikit\u2011learn\u2019s <code>AgglomerativeClustering</code>.</p>"},{"location":"agglomerative-technical/#introduction","title":"Introduction","text":"<p>Hierarchical clustering builds nested partitions by iteratively merging or splitting clusters. Agglomerative clustering starts with each point as its own cluster and merges the closest pair until <code>k</code> clusters remain. Implementing this in MeTTa:</p> <ul> <li>Demonstrates MeTTa\u2019s capability to express non\u2010parametric, recursive algorithms.  </li> <li>Provides a template for tree\u2010based clustering methods.  </li> <li>Targets AI and Data Science open\u2011source contributors.</li> </ul> <p>MeTTa is a multi\u2011paradigm language for functional and declarative computations over meta\u2011graphs. See http://www.metta\u2011lang.dev.</p>"},{"location":"agglomerative-technical/#algorithm-overview","title":"Algorithm Overview","text":"<ol> <li>Initialization: Start with <code>n</code> singleton clusters.  </li> <li>Distance Matrix: Compute all pairwise distances.  </li> <li>Linkage: Define cluster distance via single (min), complete (max), or average metrics.  </li> <li>Merge Loop: Repeatedly find and merge the two closest clusters until <code>k</code> remain.  </li> <li>Assignment: Label each data point by its final cluster.</li> </ol>"},{"location":"agglomerative-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa code uses recursive definitions and NumPy bindings (<code>metta_ul</code>) plus a linked\u2010list module for cluster lists.</p> <ol> <li>Cluster Initialization (<code>agglomerative.init-clusters</code>):</li> <li>Recursively builds a <code>List</code> of singleton <code>PyList</code>s containing each index from <code>0</code> to <code>n-1</code>.</li> <li> <p>Time:\u00a0O(n) \u00a0Memory:\u00a0O(n)</p> </li> <li> <p>Distance Matrix Computation (<code>agglomerative.distance-matrix</code>):</p> </li> <li>Computes an n-by-n symmetric matrix of Euclidean distances using <code>np.expand_dims</code> and <code>np.linalg.norm</code> over the last axis.</li> <li> <p>Time:\u00a0O(n\u00b2\u00b7d)\u00a0\u00a0Memory:\u00a0O(n\u00b2)</p> </li> <li> <p>Linkage Distance (<code>agglomerative.linkage-distance</code>):</p> </li> <li>For clusters C1, C2, extracts submatrix of pairwise distances and applies:<ul> <li>single: min\u00a0\u00a0complete: max\u00a0\u00a0average: mean</li> </ul> </li> <li> <p>Time:\u00a0O(|C1|\u00b7|C2|)\u00a0\u00a0Memory:\u00a0O(|C1|\u00b7|C2|)</p> </li> <li> <p>Closest Cluster Pair (<code>agglomerative.closest-clusters</code>):</p> </li> <li>Recursively iterates all cluster pairs via two\u2010list traversal, tracking the minimum linkage distance.</li> <li>Time:\u00a0O(m\u00b2\u00b7p) where m=current cluster count, p=cluster\u2010pair cost; worst\u2010case O(n\u2074) but decreases as clusters merge.  </li> <li> <p>Memory: negligible extra beyond clusters list.</p> </li> <li> <p>Merge Clusters (<code>agglomerative.merge-clusters</code>):</p> </li> <li>Uses <code>closest-clusters</code> to identify clusters (c1, c2), concatenates their index lists, and updates the cluster <code>List</code>.</li> <li> <p>Time &amp; Memory: dominated by <code>closest-clusters</code>, plus O(|c1|+|c2|) list operations.</p> </li> <li> <p>Recursive Merge Loop (<code>agglomerative.recursion</code>):</p> </li> <li>Recurses until the number of clusters equals <code>k</code>, performing <code>n-k</code> merges.</li> <li>Time: O((n\u2013k)\u00b7n\u2074) worst\u2011case; typical much lower.  </li> <li> <p>Memory: tail-call optimized, peak ~O(n\u00b2).</p> </li> <li> <p>Assignment (<code>agglomerative.assign</code>):</p> </li> <li>Traverses final clusters and fills an assignment vector of length <code>n</code> with cluster indices.</li> <li> <p>Time:\u00a0O(n)\u00a0\u00a0Memory:\u00a0O(n)</p> </li> <li> <p>Fit &amp; Predict API (<code>agglomerative.fit-predict</code>):</p> </li> <li>One\u2010line pipeline: <code>(agglomerative.fit-predict X k linkage)</code> returns an <code>np.array</code> of labels.</li> <li>Time: combined complexities above. \u00a0Memory: O(n\u00b2).</li> </ol>"},{"location":"agglomerative-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn\u2019s <code>AgglomerativeClustering</code> implements similar linkage strategies with optimized C loops and optional connectivity constraints.</p> Feature MeTTa Implementation scikit\u2011learn Linkage types single, complete, average ward, complete, average, single Connectivity not supported graph\u2010based sparse Initialization trivial (singletons) trivial (singletons) Convergence control fixed merges to reach <code>k</code> same Parallelization single\u2011threaded single\u2011threaded Memory trade\u2011off stores full distance matrix stores condensed or sparse matrices"},{"location":"agglomerative-technical/#benchmark-setup","title":"Benchmark Setup","text":"<p>(To be populated with dataset-specific benchmarks.)</p>"},{"location":"agglomerative-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul:cluster:agglomerative)\n\n;; Fit and predict 4 clusters with average linkage\n(let $labels (agglomerative.fit-predict X 4 \"average\")\n    (println! $labels)\n)\n</code></pre>"},{"location":"agglomerative-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>High complexity: worst-case merging is expensive; consider optimized search (e.g., priority queue).  </li> <li>Connectivity constraints: add sparse linkage for large datasets.  </li> <li>Parallel merging: explore MeTTa\u2019s concurrent constructs.  </li> </ul>"},{"location":"agglomerative-technical/#conclusion","title":"Conclusion","text":"<p>This implementation illustrates MeTTa\u2019s ability to encode hierarchical clustering declaratively. While performance and memory usage are constrained by O(n\u00b2) matrices and recursive merging, it provides a clear foundation for advanced clustering features.</p>"},{"location":"agglomerative-technical/#references","title":"References","text":"<ol> <li>Johnson (1967). Hierarchical clustering schemes.  </li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.  </li> <li>MeTTa Language Specification. http://www.metta-lang.dev/spec</li> </ol>"},{"location":"bisecting-kmeans-api-reference/","title":"metta_ul:cluster:bisecting-kmeans","text":""},{"location":"bisecting-kmeans-api-reference/#overview","title":"Overview","text":"<p>This module implements the Bisecting K-means clustering algorithm using iterative splitting of clusters. The algorithm starts with the entire dataset as a single cluster and recursively bisects the cluster with the maximum Sum of Squared Errors (SSE) using standard k-means (with k=2) until a desired number of clusters is reached. The resulting hierarchical clustering structure captures the splits performed during the process.</p>"},{"location":"bisecting-kmeans-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscompute-sse","title":"<code>bisecting-kmeans.compute-sse</code>","text":"<p>Computes the Sum of Squared Errors (SSE) for a given cluster.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$indices</code>: Indices of the data points belonging to the cluster.</li> <li>Type: <code>(NPArray ($M))</code></li> <li><code>$centers</code>: The center (mean) of the cluster.</li> <li>Type: <code>(NPArray ($C))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns","title":"Returns:","text":"<ul> <li>The SSE value computed as the sum of squared differences between the data points and the cluster center.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscompute-initial-cluster","title":"<code>bisecting-kmeans.compute-initial-cluster</code>","text":"<p>Computes the initial cluster for the dataset.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A list containing the initial cluster.</li> <li>Type: <code>ClusterList</code></li> </ul> <p>Where the <code>ClusterList</code> is a list of clusters, and each cluster is a tuple containing: 1. Indices: All data point indices in <code>$X</code>.    - Type: <code>(NPArray ($N $D))</code> 2. Center: The mean of the dataset.    - Type: <code>(NPArray ($C))</code> 3. SSE: The computed SSE for the cluster.    - Type: <code>Number</code> 4. Hierarchy: <code>pyNone</code> (as no hierarchy is present initially).    - Type: <code>Hierarchy</code></p>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-indices","title":"<code>bisecting-kmeans.get-cluster-indices</code>","text":"<p>Extracts the indices from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple <code>($indices $center $sse $hierarchy)</code> representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>The indices of the data points in the cluster.</li> <li>Type: <code>(NPArray ($M))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-center","title":"<code>bisecting-kmeans.get-cluster-center</code>","text":"<p>Extracts the center of the cluster.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>The center of the cluster.</li> <li>Type: <code>(NPArray ($C))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-sse","title":"<code>bisecting-kmeans.get-cluster-sse</code>","text":"<p>Extracts the SSE value from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>The SSE value of the cluster.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-hierarchy","title":"<code>bisecting-kmeans.get-cluster-hierarchy</code>","text":"<p>Extracts the hierarchical structure information from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_5","title":"Returns:","text":"<ul> <li>The hierarchy associated with the cluster.</li> <li>Type: <code>Hierarchy</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansfind-max-cluster","title":"<code>bisecting-kmeans.find-max-cluster</code>","text":"<p>Finds the cluster with the maximum SSE from a list of clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li>A list of clusters, where each cluster is a tuple <code>($indices $center $sse $hierarchy)</code>.</li> <li>Type: <code>ClusterList</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_6","title":"Returns:","text":"<ul> <li>The cluster tuple with the highest SSE value.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscluster-equal","title":"<code>bisecting-kmeans.cluster-equal</code>","text":"<p>Checks whether two clusters are equal based on their indices, center, and SSE.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li>Two cluster tuples: <code>($indices1 $center1 $sse1 $hierarchy1)</code> and <code>($indices2 $center2 $sse2 $hierarchy2)</code>.</li> <li>Type: <code>Cluster Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_7","title":"Returns:","text":"<ul> <li><code>True</code> if both clusters are equal; otherwise, <code>False</code>.</li> <li>Type: <code>Bool</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansremove-cluster","title":"<code>bisecting-kmeans.remove-cluster</code>","text":"<p>Removes a target cluster from a list of clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li><code>$clusters</code>: The list of current clusters.</li> <li>Type: <code>ClusterList</code></li> <li><code>$target</code>: The cluster tuple to be removed.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_8","title":"Returns:","text":"<ul> <li>An updated list of clusters with the target cluster removed.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansbisect-cluster","title":"<code>bisecting-kmeans.bisect-cluster</code>","text":"<p>Performs bisection on a given cluster using standard k-means with k=2.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_9","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$max-cluster</code>: The cluster to be bisected, represented as a tuple.</li> <li>Type: <code>Cluster</code></li> <li><code>$max-iter</code>: The maximum number of iterations allowed for the k-means algorithm during the bisecting process.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_9","title":"Returns:","text":"<ul> <li>A tuple containing two clusters obtained from splitting the input cluster. Each cluster is represented as <code>(indices, center, sse, hierarchy)</code>.</li> <li>Type: <code>ClusterList</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansrecursive-bisecting-kmeans","title":"<code>bisecting-kmeans.recursive-bisecting-kmeans</code>","text":"<p>Recursively applies bisecting k-means to further split clusters until the desired number of clusters is reached.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_10","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$clusters</code>: The current list of clusters.</li> <li>Type: <code>ClusterList</code></li> <li><code>$max-num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$max-iter</code>: The maximum iterations for each bisecting step.</li> <li>Type: <code>Number</code></li> <li><code>$hierarchy</code>: The current hierarchical clustering structure maintained as a MeTTa list.</li> <li>Type: <code>Hierarchy</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_10","title":"Returns:","text":"<ul> <li>An updated hierarchical clustering structure as a MeTTa list describing the clustering process.</li> <li>Type: <code>Hierarchy</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansfit","title":"<code>bisecting-kmeans.fit</code>","text":"<p>Performs bisecting k-means clustering on the dataset.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_11","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$max-num-clusters</code>: The maximum (desired) number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm during the bisecting process.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_11","title":"Returns:","text":"<ul> <li>A hierarchical clustering structure as a MeTTa list that describes the sequence of splits performed.</li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansassign-point-to-cluster","title":"<code>bisecting-kmeans.assign-point-to-cluster</code>","text":"<p>Assigns a single data point to the closest cluster based on Euclidean distance.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_12","title":"Parameters:","text":"<ul> <li><code>$point</code>: A single data point from the dataset.</li> <li>Type: <code>(NPArray ($D))</code></li> <li><code>$clusters</code>: A list of clusters.</li> <li>Type: <code>ClusterList</code></li> <li><code>$best-cluster-idx</code>: The current best cluster index for the point (initial value provided).</li> <li>Type: <code>Number</code></li> <li><code>$best-distance</code>: The current best distance found (initially set to a high value, such as <code>pyINF</code>).</li> <li>Type: <code>Number</code></li> <li><code>$cluster-idx</code>: The current index being evaluated.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_12","title":"Returns:","text":"<ul> <li>The index of the cluster that is closest to the given data point.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansassign-all-points","title":"<code>bisecting-kmeans.assign-all-points</code>","text":"<p>Assigns all data points in the dataset to their closest clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_13","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($D))</code></li> <li><code>$clusters</code>: The list of clusters.</li> <li>Type: <code>ClusterList</code></li> <li><code>$point-idx</code>: The current index of the data point being processed.</li> <li>Type: <code>Number</code></li> <li><code>$labels</code>: A list of cluster labels corresponding to the assignment of each data point.</li> <li>Type: <code>LabelList</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_13","title":"Returns:","text":"<ul> <li>A list of cluster labels indicating the assignment of each data point in <code>$X</code> to the nearest cluster.</li> <li>Type: <code>LabelList</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanspredict","title":"<code>bisecting-kmeans.predict</code>","text":"<p>Predicts the cluster membership for each data point based on the final hierarchical clustering structure.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_14","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$hierarchy</code>: The hierarchical clustering structure generated by <code>bisecting-kmeans.fit</code>.</li> <li>Type: <code>Hierarchy</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_14","title":"Returns:","text":"<ul> <li>A list of cluster labels indicating the assigned cluster for each data point in the dataset.</li> <li>Type: <code>LabelList</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#usage","title":"Usage","text":"<p>To perform bisecting k-means clustering on a dataset <code>S</code> (an <code>NPArray</code> of shape <code>(n, d)</code>) with a maximum of 5 clusters and 100 maximum iterations for k-means: <pre><code>(=\n    (clustering-history)\n    (bisecting-kmeans.fit (S) 5 100)\n)\n</code></pre> After clustering, you can predict the cluster assignments for S as follows: <pre><code>(=\n    (cluster-labels)\n    (bisecting-kmeans.predict (S) (clustering-history))\n)\n</code></pre></p>"},{"location":"bisecting-kmeans-api-reference/#notes","title":"Notes","text":"<ul> <li> <p>The algorithm repeatedly splits clusters by applying standard k-means with k=2,  targeting the cluster with the highest SSE for bisection.</p> </li> <li> <p>The clustering process maintains a hierarchical structure that records the  splits performed, which can be used for further analysis or visualization.</p> </li> <li> <p>SSE is used as the criterion for selecting the cluster to bisect,  ensuring that the cluster with the largest dispersion is split at each iteration.</p> </li> </ul>"},{"location":"bisecting-kmeans-technical/","title":"Bisecting K-means in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 24, 2025 Version: 1.0  </p>"},{"location":"bisecting-kmeans-technical/#abstract","title":"Abstract","text":"<p>This report presents an implementation of Bisecting K-means clustering in the MeTTa language. Bisecting K-means is a hierarchical variant of the standard K-means algorithm that recursively divides clusters through binary splits. Our implementation constructs a complete divisive clustering solution, maintaining a hierarchical structure of the resulting partitions. Benchmarks show competitive clustering quality with runtimes of ~4.9s, offering a robust hierarchical approach that's conceptually simpler than agglomerative methods. This work demonstrates MeTTa's capability to express recursive algorithms with complex data structures while highlighting both strengths and areas for optimization in hierarchical clustering.</p>"},{"location":"bisecting-kmeans-technical/#introduction","title":"Introduction","text":"<p>Bisecting K-means is a divisive hierarchical clustering approach that offers several advantages over standard K-means and agglomerative methods:</p> <ul> <li>Produces a hierarchical tree structure that represents clustering at multiple levels</li> <li>Generally more efficient than agglomerative approaches for large datasets</li> <li>Often creates more balanced clusters than standard K-means</li> <li>Follows a top-down approach that can be more intuitive for certain applications</li> </ul> <p>Implementing Bisecting K-means in MeTTa:</p> <ul> <li>Demonstrates MeTTa's capability to implement recursive algorithms</li> <li>Showcases MeTTa's ability to represent hierarchical data structures</li> <li>Extends MeTTa's ML toolkit with a hierarchical clustering approach</li> <li>Targets open\u2011source users in AI and Data Science</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative and functional computations over knowledge metagraphs. See http://www.metta\u2011lang.dev for details.</p>"},{"location":"bisecting-kmeans-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>Bisecting K-means operates by recursively splitting clusters until the desired number of clusters is reached. The core strategy follows these steps:</p> <ol> <li>Initialization: Begin with all data points in a single cluster</li> <li>Iterative Bisection:</li> <li>Select the cluster with the largest Sum of Squared Errors (SSE)</li> <li>Apply standard K-means with k=2 to divide this cluster into two subclusters</li> <li>Add the two resulting clusters to the cluster list</li> <li>Remove the original cluster from the list</li> <li>Termination: Continue until the desired number of clusters is reached</li> </ol> <p>The algorithm leverages the divisive hierarchical approach, which starts with all points in one cluster and recursively splits clusters. The key insight is that by always splitting the cluster with the largest SSE, we prioritize improving the clustering quality by focusing on the most heterogeneous clusters first.</p>"},{"location":"bisecting-kmeans-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa implementation organizes clusters as tuples of (indices, center, SSE, hierarchy) and uses recursion for the bisecting process:</p> <ol> <li>Cluster Representation    Each cluster is represented as a tuple containing:    <pre><code>($indices $center $sse $hierarchy)\n</code></pre></li> <li><code>$indices</code>: Array of indices for data points in the cluster</li> <li><code>$center</code>: Centroid of the cluster</li> <li><code>$sse</code>: Sum of squared errors within the cluster</li> <li> <p><code>$hierarchy</code>: Hierarchical structure of subclusters</p> </li> <li> <p>Initial Cluster Creation    Begins with all data points in a single cluster:    <pre><code>(bisecting-kmeans.compute-initial-cluster $X)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7d) for computing the initial mean</li> <li> <p>Space Complexity: O(n) for storing indices</p> </li> <li> <p>Finding Cluster to Split    Selects the cluster with the largest SSE:    <pre><code>(bisecting-kmeans.find-max-cluster $clusters)\n</code></pre></p> </li> <li>Time Complexity: O(k) where k is the current number of clusters</li> <li> <p>Space Complexity: O(1) for storing the max cluster</p> </li> <li> <p>Bisecting a Cluster    Applies K-means with k=2 to split the selected cluster:    <pre><code>(bisecting-kmeans.bisect-cluster $X $max-cluster $max-iter)\n</code></pre></p> </li> <li>Time Complexity: O(t\u00b7n'\u00b7d) where t is K-means iterations and n' is points in the cluster</li> <li> <p>Space Complexity: O(n') for storing the new cluster assignments</p> </li> <li> <p>Recursive Bisecting Process    Recursively applies the bisecting process until reaching the desired number of clusters:    <pre><code>(bisecting-kmeans.recursive-bisecting-kmeans $X $clusters $max-num-clusters $max-iter $hierarchy)\n</code></pre></p> </li> <li>Time Complexity: O(k\u00b7t\u00b7n\u00b7d) where k is the target number of clusters</li> <li> <p>Space Complexity: O(k\u00b7n) for storing all cluster hierarchies</p> </li> <li> <p>Complete Fitting Function    Orchestrates the complete bisecting K-means algorithm:    <pre><code>(bisecting-kmeans.fit $X $max-num-clusters $max-kmeans-iter)\n</code></pre></p> </li> <li>Overall Time Complexity: O(k\u00b7t\u00b7n\u00b7d)</li> <li> <p>Overall Space Complexity: O(k\u00b7n)</p> </li> <li> <p>Prediction Function    Assigns new data points to clusters based on closest centroids:    <pre><code>(bisecting-kmeans.predict $X $hierarchy)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for assignment calculation</li> <li>Space Complexity: O(n) for assignment array</li> </ol> <p>The implementation handles recursive clustering through functional decomposition, with the hierarchical structure explicitly maintained during the bisecting process.</p>"},{"location":"bisecting-kmeans-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn's <code>BisectingKMeans</code> (introduced in version 1.1) offers some optimizations compared to our MeTTa implementation:</p> Feature MeTTa Implementation scikit\u2011learn Cluster selection Largest SSE only Largest SSE or random Bisection method Standard K-means K-means or K-means++ Parallel execution None None (no n_jobs parameter) Hierarchy tracking Full hierarchy maintained Final clusters only Memory efficiency Stores all intermediate clusters More memory-efficient implementation Implementation Recursive with functional approach Iterative with optimized C/Cython <p>The MeTTa implementation provides explicit hierarchy tracking that scikit\u2011learn doesn't expose, while scikit\u2011learn offers better performance optimizations.</p>"},{"location":"bisecting-kmeans-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each, synthetic generation via scikit\u2011learn):</li> <li><code>noisy_circles</code>: Two concentric circles</li> <li><code>noisy_moons</code>: Two interleaving half\u2011circles</li> <li><code>varied</code>: Varied variance blobs</li> <li>Environment: CPU = 4 cores @ 3.0 GHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1</li> </ul>"},{"location":"bisecting-kmeans-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset MeTTa Time (s) scikit\u2011learn Time (s) Silhouette Calinski\u2011Harabasz Davies\u2011Bouldin ARI NMI AMI blobs 3.036 &lt; 0.01 0.6541 1423.97 0.4787 0.9821 0.9691 0.9690 noisy_moon 1.541 &lt; 0.01 0.4956 690.81 0.8120 0.4834 0.3857 0.3848 no_structure 3.244 &lt; 0.01 0.3629 356.97 0.8715 0.0000 0.0000 0.0000 varied 2.996 &lt; 0.01 0.6396 1548.48 0.6096 0.7266 0.7313 0.7303 noisy_circles 1.129 &lt; 0.01 0.3496 282.17 1.1915 \u22120.0019 0.0001 \u22120.0014"},{"location":"bisecting-kmeans-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul) \n(import! &amp;self bisecting-kmeans)\n\n(let \n    $labels \n    (bisecting-kmeans.predict X $hierarchy)\n    (println! $labels)\n)\n</code></pre> <p>This snippet: 1. Imports required modules 2. Fits Bisecting K-means with 2 clusters and max 10 iterations for each K-means application 3. Predicts cluster assignments 4. Prints the resulting labels</p>"},{"location":"bisecting-kmeans-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Non-convex Clusters: As with standard K-means, Bisecting K-means struggles with non-convex cluster shapes, as shown in the benchmark results for <code>noisy_circles</code>.</li> <li>Cluster Selection Strategy: Currently only selects the cluster with largest SSE; could be extended with different selection criteria like random selection or cluster size.</li> <li>Initialization Method: Uses standard K-means for bisection; could benefit from K-means++ initialization.</li> <li>Memory Efficiency: Stores the complete hierarchy which could be memory-intensive for large datasets with many clusters.</li> <li>Termination Criteria: Currently only terminates based on number of clusters; could add additional criteria based on minimum SSE improvement.</li> </ul>"},{"location":"bisecting-kmeans-technical/#conclusion","title":"Conclusion","text":"<p>The MeTTa implementation of Bisecting K-means demonstrates the language's capability to express recursive algorithms and hierarchical data structures. While the algorithm doesn't outperform Spectral Clustering on complex geometries, it provides valuable hierarchical information not available in flat clustering methods. The functional and declarative approach in MeTTa makes the algorithm structure clear and maintainable, though with some performance trade-offs compared to optimized implementations.</p> <p>Bisecting K-means represents an important middle ground between simple partitional methods like K-means and more complex techniques like Spectral Clustering, making it a valuable addition to the MeTTa machine learning toolkit, particularly for applications where hierarchical structure is important and clusters are relatively convex.</p>"},{"location":"bisecting-kmeans-technical/#references","title":"References","text":"<ol> <li>Steinbach, M., Karypis, G., &amp; Kumar, V. (2000). A comparison of document clustering techniques.</li> <li>Savaresi, S. M., &amp; Boley, D. L. (2004). A comparative analysis on the bisecting K-means and the PDDP clustering algorithms.</li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.</li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"gmm-api-reference/","title":"metta_ul:cluster:gmm","text":""},{"location":"gmm-api-reference/#overview","title":"Overview","text":"<p>This module implements the Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm in MeTTa. GMM is a probabilistic model for representing normally distributed subpopulations within an overall dataset.</p>"},{"location":"gmm-api-reference/#functions","title":"Functions","text":""},{"location":"gmm-api-reference/#gmmcenter","title":"<code>gmm.center</code>","text":"<p>Centers the data points by subtracting the means.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code></p> <p>Returns: - Centered data matrix.     - Type: <code>(NPArray ($n $k $d))</code></p>"},{"location":"gmm-api-reference/#gmmmahalanobis-term","title":"<code>gmm.mahalanobis-term</code>","text":"<p>Computes the Mahalanobis distance term for the Gaussian probability density function.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Mahalanobis distance matrix.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmgaussian-pdf","title":"<code>gmm.gaussian-pdf</code>","text":"<p>Computes the probability density function (PDF) for a multivariate Gaussian distribution.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Matrix of Gaussian probabilities for each point and cluster.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmlog-likelihood","title":"<code>gmm.log-likelihood</code>","text":"<p>Computes the log-likelihood of the dataset given the current GMM parameters.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$weights</code>: Mixture component weights.     - Type: <code>(NPArray ($k))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Log-likelihood scalar value.     - Type: <code>Number</code></p>"},{"location":"gmm-api-reference/#gmminit","title":"<code>gmm.init</code>","text":"<p>Initializes the GMM parameters (weights, means, and covariances).</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$k</code>: Number of Gaussian components.</p> <p>Returns: - Initial weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmme-step","title":"<code>gmm.e-step</code>","text":"<p>Performs the Expectation (E) step of the EM algorithm, computing responsibilities.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$weights</code>: Mixture component weights.     - Type: <code>(NPArray ($k))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Responsibility matrix.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmm-step","title":"<code>gmm.m-step</code>","text":"<p>Performs the Maximization (M) step of the EM algorithm, updating parameters.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$responsibilities</code>: Responsibility matrix from the E-step.     - Type: <code>(NPArray ($n $k))</code></p> <p>Returns: - Updated weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmmrecursion","title":"<code>gmm.recursion</code>","text":"<p>Recursively applies the EM steps until the maximum number of iterations is reached.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>($weights, $means, $covariances)</code>: Current GMM parameters.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code> - <code>$max-iter</code>: Maximum number of iterations.     - Type: <code>Number</code></p> <p>Returns: - Final weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmm","title":"<code>gmm</code>","text":"<p>Main function to train a GMM on a dataset.</p> <p>Parameters - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$k</code>: Count of components.     - Type: <code>Number</code> - <code>$max-iter</code>: Maximum number of iterations.     - Type: <code>Number</code></p> <p>Returns: - Final weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#usage","title":"Usage","text":"<p>To cluster a dataset <code>S</code> of type <code>(NPArray ($n $d))</code> into 3 clusters with default settings: <pre><code>(=\n    (params)\n    (gmm (S) 3)\n)\n</code></pre> To specify a maximum number of 50 iterations: <pre><code>(gmm (S) 3 50)\n</code></pre> To assign a dataset <code>X</code> of type <code>(NPArray ($m $d))</code> using <code>params</code>: <pre><code>(=\n    (assignments)\n    (gmm.e-step (X) (params))\n)\n</code></pre></p>"},{"location":"gmm-api-reference/#notes","title":"Notes","text":"<ul> <li>The GMM parameters are initialized using random means selected from the dataset and a slightly perturbed covariance matrix.</li> <li>The EM algorithm iteratively refines the parameters to maximize the log-likelihood of the data.</li> <li>Responsibilities indicate the probability of each data point belonging to each cluster.</li> </ul>"},{"location":"gmm-api-reference/#dependencies","title":"Dependencies","text":"<p>This module relies on the following NumPy operations within MeTTa: - <code>np.sub</code>: Element-wise subtraction. - <code>np.einsum</code>: Einstein summation notation for efficient matrix operations. - <code>np.linalg.inv</code>: Inversion of covariance matrices. - <code>np.linalg.slogabsdet</code>: Log determinant of covariance matrices. - <code>np.mul</code>: Element-wise multiplication. - <code>np.add</code>: Element-wise addition. - <code>np.sum</code>: Summation along an axis. - <code>np.log</code>: Logarithm function. - <code>np.div</code>: Element-wise division. - <code>np.exp</code>: Exponential function. - <code>np.ones</code>: Creates an array of ones. - <code>np.repeat</code>: Repeats an array along a specified axis. - <code>np.eye</code>: Creates an identity matrix. - <code>np.choose</code>: Selects random initial means from the dataset. - <code>np.cov</code>: Computes the covariance matrix.</p> <p>This implementation provides a full pipeline for training a Gaussian Mixture Model using the EM algorithm in MeTTa.</p>"},{"location":"gmm-technical/","title":"Gaussian Mixture Model (GMM) Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzedeh Date: May 21, 2025 Version: 1.0</p>"},{"location":"gmm-technical/#abstract","title":"Abstract","text":"<p>This report details an open\u2011source implementation of the Gaussian Mixture Model (GMM) clustering algorithm in the MeTTa language. It showcases MeTTa\u2019s declarative and functional strengths by expressing the Expectation\u2013Maximization steps\u2014without native loops\u2014using recursive function definitions and NumPy bindings. Performance considerations and complexity analyses are discussed in the context of future improvements.</p>"},{"location":"gmm-technical/#introduction","title":"Introduction","text":"<p>Probabilistic clustering via GMM models each cluster as a multivariate Gaussian, allowing soft assignments and richer cluster shapes compared to KMeans. Embedding GMM in MeTTa:</p> <ul> <li>Extends MeTTa\u2019s numeric capabilities with probabilistic models.  </li> <li>Provides a template for future statistical algorithm implementations.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative computations over knowledge graphs. See http://www.metta-lang.dev for more information.</p>"},{"location":"gmm-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>GMM uses the Expectation\u2013Maximization (EM) algorithm to maximize data likelihood under a mixture of Gaussians:</p> <ol> <li>E\u2011Step: Compute responsibilities $r_{ik} = p(z_k|x_i)$ via Gaussian PDFs and mixture weights.  </li> <li>M\u2011Step: Update weights $\\pi_k$, means $\\mu_k$, and covariances $\\Sigma_k$ based on responsibilities.  </li> <li>Repeat until convergence or max iterations.</li> </ol> <p>Mathematically:</p> <ul> <li>Gaussian PDF: <pre><code>p_k(x) = \\exp\\bigl(-\\tfrac12 (x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) - \\tfrac{d}{2}\\ln(2\\pi) - \\tfrac12 \\ln|\\Sigma_k|\\bigr)\n</code></pre></li> <li>Log\u2011Likelihood: <pre><code>  L = \\sum_{i=1}^n \\ln \\Bigl( \\sum_{k=1}^K \\pi_k p_k(x_i) \\Bigr)\n</code></pre></li> </ul>"},{"location":"gmm-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The GMM implementation uses MeTTa function definitions with NumPy bindings:</p> <ol> <li>Centering    Subtract means from data via broadcasting.  </li> <li>Time Complexity: O(n\u00b7k\u00b7d).  </li> <li> <p>Memory: O(n\u00b7k\u00b7d).</p> </li> <li> <p>Mahalanobis Term    Compute $(x_i-\\mu_k)^T \\Sigma_k^{-1}(x_i-\\mu_k)$ with <code>np.einsum</code>.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d^2).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Gaussian PDF    Evaluate the multivariate Gaussian density using the Mahalanobis term, determinant, and normalization.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d^2).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Log\u2011Likelihood    Sum log of weighted PDFs across data points.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Initialization (<code>gmm.init</code>) </p> </li> <li>Weights: uniform $\\frac{1}{K}$.  </li> <li>Means: random points via <code>np.choose</code>.  </li> <li>Covariances: data covariance + small identity noise.  </li> <li>Time Complexity: O(n\u00b7d^2).  </li> <li> <p>Memory: O(k\u00b7d^2).</p> </li> <li> <p>E\u2011Step (<code>gmm.e-step</code>)    Compute responsibilities $r_{ik} = \\pi_k p_k(x_i) / \\sum_j \\pi_j p_j(x_i)$.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>M\u2011Step (<code>gmm.m-step</code>)    Update:</p> </li> <li>$N_k = \\sum_i r_{ik}$</li> <li>$\\pi_k = N_k / n$</li> <li>$\\mu_k = (1/N_k) \\sum_i r_{ik} x_i$</li> <li> <p>$\\Sigma_k = (1/N_k) \\sum_i r_{ik} (x_i-\\mu_k)(x_i-\\mu_k)^T$  </p> </li> <li> <p>Time Complexity: O(n\u00b7k\u00b7d^2).  </p> </li> <li> <p>Memory: O(k\u00b7d^2 + n\u00b7k).</p> </li> <li> <p>Recursive EM Loop    Repeat E\u2011 and M\u2011Steps up to <code>max_iter</code>.  </p> </li> <li>Time Complexity: O(max_iter \u00b7 n \u00b7 k \u00b7 d^2).  </li> <li> <p>Memory: Tail\u2011calls reuse frames; peak O(n\u00b7k\u00b7d^2).</p> </li> <li> <p>Predict (<code>gmm.predict</code>)    Assign cluster by highest responsibility.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li>Memory: O(n\u00b7k).</li> </ol>"},{"location":"gmm-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn provides <code>GaussianMixture</code> with C-optimized loops and multiple covariance options. Future work may include:</p> Feature MeTTa Implementation scikit\u2011learn Covariance types full only full, tied, diag, spherical Initialization random k\u2011means++, random Convergence control tol on log\u2011likelihood tol on log\u2011likelihood Parallelization single\u2011threaded single\u2011threaded"},{"location":"gmm-technical/#benchmark-setup","title":"Benchmark Setup","text":"Dataset MeTTa Time (s) scikit-learn Time (s) Silhouette Calinski-Harabasz Davies-Bouldin Adjusted Rand Index Normalized Mutual Info Adjusted Mutual Info varied 15.125 &lt;0.01 0.58807 1198.59 0.68460 0.94682 0.91605 0.91574 blobs 4.519 &lt;0.01 0.65379 1422.54 0.47993 0.96444 0.94787 0.94768 noisy_circles 149.801 &lt;0.01 0.34909 281.92 1.19307 -0.00199 0.00001 -0.00144 noisy_moon 78.848 &lt;0.01 0.35369 297.61 0.88993 0.23313 0.33222 0.33114 no-structure 151.017 &lt;0.01 0.22208 181.17 1.60891 0 0 0"},{"location":"gmm-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul:cluster:gmm)\n\n(let $params (gmm.fit X 3 50)\n  (println! (gmm.predict X $params))\n)\n</code></pre>"},{"location":"gmm-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Support additional covariance structures.</li> </ul>"},{"location":"gmm-technical/#conclusion","title":"Conclusion","text":"<p>The GMM implementation demonstrates MeTTa\u2019s ability to express probabilistic EM algorithms declaratively. While performance depends on NumPy backends, the clear, recursive definitions simplify future extensions.</p>"},{"location":"gmm-technical/#references","title":"References","text":"<ol> <li>Dempster, Laird &amp; Rubin (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm.  </li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.  </li> <li>MeTTa Language Specification. http://www.metta-lang.dev/spec</li> </ol>"},{"location":"kmeans-api-reference/","title":"metta_ul:cluster:kmeans","text":""},{"location":"kmeans-api-reference/#overview","title":"Overview","text":"<p>This module implements the K-Means clustering algorithm in MeTTa. K-Means is a popular unsupervised learning method for partitioning a dataset into <code>k</code> clusters based on feature similarity.</p>"},{"location":"kmeans-api-reference/#functions","title":"Functions","text":""},{"location":"kmeans-api-reference/#kmeansupdate","title":"<code>kmeans.update</code>","text":""},{"location":"kmeans-api-reference/#description","title":"Description:","text":"<p>This function updates the centroids based on the current cluster assignments. It computes the new centroids as the mean of all data points assigned to each cluster.</p>"},{"location":"kmeans-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$assignments</code>: One-hot encoded cluster assignment matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns","title":"Returns:","text":"<ul> <li>Updated cluster centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansassign","title":"<code>kmeans.assign</code>","text":""},{"location":"kmeans-api-reference/#description_1","title":"Description:","text":"<p>Assigns each data point to the nearest centroid.</p>"},{"location":"kmeans-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$centroids</code>: Current centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A one-hot encoded assignment matrix indicating which cluster each point belongs to.<ul> <li>Type: <code>(NPArray ($k $n))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansrecursion","title":"<code>kmeans.recursion</code>","text":""},{"location":"kmeans-api-reference/#description_2","title":"Description:","text":"<p>Recursively updates the centroids until the maximum number of iterations is reached.</p>"},{"location":"kmeans-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$centroids</code>: Initial centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> <li><code>$max-iter</code>: Maximum number of iterations.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>Final cluster centroids after convergence or reaching the iteration limit.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansfit","title":"<code>kmeans.fit</code>","text":""},{"location":"kmeans-api-reference/#description_3","title":"Description:","text":"<p>Main function to perform K-Means clustering. It initializes centroids randomly and iteratively updates them.</p>"},{"location":"kmeans-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> <li><code>$k</code>: Number of clusters.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$max-iter</code> (optional, default: 100): Maximum number of iterations.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$tol</code> (optional, default: 0.0001): Tolerance on the shift of the centroids.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>Final cluster centroids after completion of the algorithm.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeanspredict","title":"<code>kmeans.predict</code>","text":""},{"location":"kmeans-api-reference/#description_4","title":"Description:","text":"<p>Main function for predicting the labels of some test data after learning the centroids from the training data. It computes the assignments and then return the positions of the maximum assignments as the labels.</p>"},{"location":"kmeans-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> <li><code>$centroids</code>: The position of centroids of the clusters.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>predicted labels.<ul> <li>Type: <code>(NPArray ($n))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#usage","title":"Usage","text":"<p>To cluster a dataset <code>S</code> of type <code>(NPArray (n, d))</code> into 3 clusters with default settings: <pre><code>(=\n    (centroids)\n    (kmeans.fit (S) 3)\n)\n</code></pre> To specify a maximum number of 50 iterations and a tolerance of 0.001: <pre><code>(kmeans (S) 3 50 0.001)\n</code></pre> To predict the labels of a dataset <code>X</code> of type <code>(NPArray (m, d))</code> using <code>centroids</code>: <pre><code>(=\n    (assignments)\n    (kmeans.predict (X) (centroids))\n)\n</code></pre></p>"},{"location":"kmeans-api-reference/#notes","title":"Notes","text":"<ul> <li>The initial centroids are chosen randomly from the data points.</li> <li>The algorithm stops after reaching the iteration limit, but convergence is not guaranteed.</li> <li>The function <code>np.one_hot</code> is used to convert the assignment indices into a one-hot encoded matrix.</li> </ul>"},{"location":"kmeans-api-reference/#dependencies","title":"Dependencies","text":"<p>This module uses NumPy functions within MeTTa: - <code>np.div</code>: Element-wise division. - <code>np.matmul</code>: Matrix multiplication. - <code>np.sum</code>: Summation along a specified axis. - <code>np.linalg.norm</code>: Computes the L2 norm. - <code>np.sub</code>: Element-wise subtraction. - <code>np.expand_dims</code>: Adds dimensions to an array. - <code>np.choose</code>: Random selection of initial centroids. - <code>np.one_hot</code>: Converts indices to one-hot encoding. - <code>np.argmin</code>: Finds the index of the minimum value along an axis.</p>"},{"location":"kmeans-technical/","title":"KMeans Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 19, 2025 Version: 1.0  </p>"},{"location":"kmeans-technical/#abstract","title":"Abstract","text":"<p>This report describes an open\u2011source implementation of the KMeans clustering algorithm in the MeTTa language. It demonstrates MeTTa\u2019s meta\u00adgraph and functional capabilities by expressing key operations\u2014assignment, update, and recursion\u2014without native loop constructs. Benchmarks on six synthetic datasets show MeTTa\u2019s runtime (~3.8\u202fs) compared to scikit\u2011learn\u2019s single\u2011threaded C backend (&lt;\u202f0.009\u202fs), highlighting trade\u2011offs between expressiveness and performance.</p>"},{"location":"kmeans-technical/#introduction","title":"Introduction","text":"<p>Clustering partitions data into groups of similar points. KMeans is a widely used algorithm that minimizes within\u2011cluster variance. Embedding clustering in MeTTa:</p> <ul> <li>Expands MeTTa\u2019s numeric and declarative expressiveness.  </li> <li>Serves as a reference for future ML algorithm implementations.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa: a multi\u2011paradigm language for declarative and functional computations over knowledge (meta)graphs. See http://www.metta-lang.dev for details.</p>"},{"location":"kmeans-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>KMeans seeks to minimize <pre><code>J = \\sum_{i=1}^{k} \\sum_{x\\in C_i} \\|x - \\mu_i\\|^2\n</code></pre></p> <p>where $\\mu_i$ are centroids. We use Euclidean distance and continue untill either a fixed maximum of recursive iterations is reached or the change in cluster centers is less than some value in MeTTa. Recursion replaces loops due to MeTTa\u2019s execution model.</p>"},{"location":"kmeans-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa KMeans implementation uses a purely declarative, recursive style to express the core steps of the algorithm without explicit loops. Data is represented as NumPy arrays wrapped in MeTTa types, and centroids, assignments, and models are built via pattern\u2010matched function definitions.</p> <ol> <li>Initialization    A fixed number of <code>k</code> centroids are sampled at random from the input matrix <code>X</code> using MeTTa\u2019s binding to NumPy\u2019s array\u2010indexing (<code>np.choose</code>).  </li> <li>Time Complexity: O(k) to sample indices.  </li> <li> <p>Memory Overhead: O(k\u00b7d) to store the initial centroids.</p> </li> <li> <p>Assignment Step    For each of the <code>n</code> data points, Euclidean distances to all <code>k</code> centroids are computed via a broadcasted norm operation. The index of the nearest centroid is converted into a one\u2010hot encoded assignment vector.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for distance calculations and argmin.  </li> <li> <p>Memory Overhead: O(n\u00b7k) for the one\u2010hot assignment matrix plus O(n\u00b7d) for intermediate expanded arrays.</p> </li> <li> <p>Update Step    Centroids are recomputed by multiplying the assignment matrix by the data matrix (<code>assignments \u00d7 X</code>) and normalizing by the cluster counts (sum over assignments).  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for matrix multiplication and O(k\u00b7n) for sum reduction.  </li> <li> <p>Memory Overhead: O(k\u00b7d) for new centroids and O(k\u00b7n) for assignment reuse.</p> </li> <li> <p>Recursive Loop with Early Stopping    The functions <code>assign</code> and <code>update</code> are composed in a recursion. Each recursive call passes the newly updated centroids to the next iteration.  </p> </li> <li>Time Complexity (worst-case): O(max_iter \u00b7 n \u00b7 k \u00b7 d).  </li> <li>Time Complexity (average-case): O(m \u00b7 n \u00b7 k \u00b7 d), where m is the iteration count until tolerance is reached (m \u2264 max_iter).  </li> <li> <p>Memory Overhead: O(n\u00b7k + k\u00b7d + n\u00b7d); recursion can use tail\u2011call optimization, so stack does not grow.</p> </li> <li> <p>Model Fit &amp; Predict </p> </li> <li><code>kmeans.fit(X, k, max_iter, tol)</code> performs recursion and returns centroids.  </li> <li><code>kmeans.predict(X, centers)</code> uses <code>argmax</code> on assignment matrix to output labels.  </li> <li>Predict Complexity: O(n\u00b7k\u00b7d) for assignment plus O(n\u00b7k) for argmax.</li> </ol> <p>The early\u2011stop mechanism often reduces iterations needed, improving average\u2011case performance while preserving worst\u2011case bounds.</p>"},{"location":"kmeans-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>Both implementations are single\u2011threaded. scikit\u2011learn\u2019s KMeans uses optimized C loops and k\u2011means++ initialization by default.</p> Feature MeTTa scikit\u2011learn Initialization random via <code>np.choose</code> k\u2011means++ Distance metric Euclidean Euclidean Control flow recursion loops (C backend) Parallelization single\u2011threaded single\u2011threaded Convergence criterion tol on centroid shift tol on centroid shift"},{"location":"kmeans-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each; seed=30 unless noted): <code>blobs</code>, <code>noisy_moons</code>, <code>noisy_circles</code>, <code>no_structure</code>, <code>aniso</code>, <code>varied</code>.  </li> <li>Synthetic generation uses scikit\u2011learn utilities (not shown).  </li> <li>Environment: CPU = 4\u202fcores @ 3.0\u202fGHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1.</li> </ul>"},{"location":"kmeans-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset MeTTa Time (s) scikit-learn Time (s) Silhouette Calinski-Harabasz Davies-Bouldin Adjusted Rand Index Normalized Mutual Info Adjusted Mutual Info varied 0.464 &lt;0.01 0.63991 1549.90 0.61256 0.74010 0.74164 0.74068 blobs 0.239 &lt;0.01 0.65424 1424.91 0.47929 0.97027 0.95445 0.95428 noisy_circles 0.797 &lt;0.01 0.35274 287.33 1.18125 -0.00186 0.00010 -0.00135 noisy_moon 0.426 &lt;0.01 0.49557 690.81 0.81197 0.48338 0.38565 0.38476 no-structure 0.986 &lt;0.01 0.38135 388.93 0.86894 0 0 0 <p>MeTTa\u2019s recursive overhead yields ~3.8\u202fs runtimes, whereas scikit\u2011learn completes in under 0.01\u202fs. Despite this gap, MeTTa\u2019s implementation showcases the language\u2019s ability to express complex numerical algorithms.</p>"},{"location":"kmeans-technical/#usage-example","title":"Usage Example","text":"<p><pre><code>(import! &amp;self metta_ul)\n\n(let $clusters (kmeans.fit X 3)\n  (println! $clusters)\n</code></pre> This snippet: 1. Imports the clustering module. 2. Fits KMeans with <code>k=3</code>. 3. Prints cluster labels.</p>"},{"location":"kmeans-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Initialization: add k\u2011means++ seeding to improve convergence.</li> </ul>"},{"location":"kmeans-technical/#conclusion","title":"Conclusion","text":"<p>This implementation verifies that MeTTa can express core ML algorithms declaratively. While performance lags optimized C libraries, the clarity and extensibility in MeTTa pave the way for further ML primitives in the language.</p>"},{"location":"kmeans-technical/#references","title":"References","text":"<ol> <li>MacQueen, J. (1967). Some Methods for Classification and Analysis of Multivariate Observations.  </li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.  </li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"spectral-api-reference/","title":"metta_ul:cluster:spectral-clustering","text":""},{"location":"spectral-api-reference/#overview","title":"Overview","text":"<p>This module implements the Spectral Clustering algorithm. It uses the RBF (Radial Basis Function) kernel to construct an affinity matrix, computes the normalized graph Laplacian, and performs eigen-decomposition to extract spectral embeddings. These embeddings are then normalized and clustered using k-means. The algorithm is designed to cluster data that may not be linearly separable, leveraging the eigenstructure of the Laplacian.</p>"},{"location":"spectral-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"spectral-api-reference/#spectral-clusteringsquare-norm","title":"<code>spectral-clustering.square-norm</code>","text":"<p>Computes the square norm for each row (data point) in the dataset.</p>"},{"location":"spectral-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns","title":"Returns:","text":"<ul> <li>A vector where each element is the sum of squares of the components of the corresponding data point.</li> <li>Type: <code>(NPArray ($N 1)</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringsquare-distance-matrix","title":"<code>spectral-clustering.square-distance-matrix</code>","text":"<p>Computes the matrix of squared Euclidean distances between each pair of data points in the dataset.</p>"},{"location":"spectral-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$square-norm-X</code>: The vector of square norms computed from <code>$X</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> <li><code>$X</code>: The dataset.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A square matrix where the entry at (i, j) is the squared Euclidean distance between data point i and j.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringrbf-affinity-matrix","title":"<code>spectral-clustering.rbf-affinity-matrix</code>","text":"<p>Generates the RBF (Radial Basis Function) affinity matrix from a squared distance matrix.</p>"},{"location":"spectral-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$sqr-distance-matrix-X</code>: The matrix of squared distances between data points.</li> <li>Type: <code>(NPArray ($N $N))</code></li> <li><code>$rbf-kernel-sigma</code>: The sigma parameter for the RBF kernel.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>A symmetric affinity matrix where each entry represents the similarity between a pair of data points based on the RBF kernel.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringdegree","title":"<code>spectral-clustering.degree</code>","text":"<p>Computes the degree of each node (data point) based on the affinity matrix.</p>"},{"location":"spectral-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$W</code>: The affinity matrix.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>A vector where each element is the sum of the corresponding row in <code>$W</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringinverse-degree-matrix","title":"<code>spectral-clustering.inverse-degree-matrix</code>","text":"<p>Constructs the inverse degree matrix used for normalization.</p>"},{"location":"spectral-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$degree-W</code>: The vector of node degrees computed from <code>$W</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>A diagonal matrix where each diagonal element is the inverse square root of the corresponding degree.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringnormalized-laplacian","title":"<code>spectral-clustering.normalized-laplacian</code>","text":"<p>Computes the normalized graph Laplacian from the affinity matrix and its inverse degree matrix.</p>"},{"location":"spectral-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li><code>$W</code>: The RBF affinity matrix.</li> <li>Type: <code>(NPArray ($N $N))</code></li> <li><code>$inverse-degree-matrix-W</code>: The inverse degree matrix computed from <code>$W</code>.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_5","title":"Returns:","text":"<ul> <li>The normalized Laplacian matrix defined as I - D^{-1/2} W D^{-1/2}, where I is the identity matrix.</li> <li>Type: (NPArray ($N $N))</li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigh","title":"<code>spectral-clustering.eigh</code>","text":"<p>Performs eigen-decomposition on a given matrix.</p>"},{"location":"spectral-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li><code>$X</code>: The matrix to decompose (typically the normalized Laplacian).</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_6","title":"Returns:","text":"<ul> <li>A tuple containing the eigenvalues and eigenvectors of <code>$X</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigenvalues","title":"<code>spectral-clustering.eigenvalues</code>","text":"<p>Extracts the eigenvalues from the result of the eigen-decomposition.</p>"},{"location":"spectral-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li><code>$eigh-X</code>: The tuple returned from <code>spectral-clustering.eigh</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#returns_7","title":"Returns:","text":"<ul> <li>A vector containing the eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigenvectors","title":"<code>spectral-clustering.eigenvectors</code>","text":"<p>Extracts the eigenvectors from the result of the eigen-decomposition.</p>"},{"location":"spectral-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li><code>$eigh-X</code>: The tuple returned from <code>spectral-clustering.eigh</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#returns_8","title":"Returns:","text":"<ul> <li>A matrix whose columns correspond to the eigenvectors of <code>$X</code>.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigval-top-k-index","title":"<code>spectral-clustering.eigval-top-k-index</code>","text":"<p>Finds the indices corresponding to the smallest k eigenvalues (after sorting).</p>"},{"location":"spectral-api-reference/#parameters_9","title":"Parameters:","text":"<ul> <li><code>$eigval-L</code>: The vector of eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> <li><code>$k</code>: The number of top eigenvalue indices to select.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_9","title":"Returns:","text":"<ul> <li>A vector of indices for the top k eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringspectral-embeddings","title":"<code>spectral-clustering.spectral-embeddings</code>","text":"<p>Computes the spectral embeddings by selecting the top k eigenvectors based on their eigenvalues.</p>"},{"location":"spectral-api-reference/#parameters_10","title":"Parameters:","text":"<ul> <li><code>$eigh-I</code>: The eigen-decomposition result of the normalized Laplacian.</li> <li>Type: <code>EighResult</code></li> <li><code>$k</code>: The number of clusters (and hence dimensions for the embeddings).</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_10","title":"Returns:","text":"<ul> <li>A matrix of spectral embeddings extracted from the selected eigenvectors.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringrow-normalize","title":"<code>spectral-clustering.row-normalize</code>","text":"<p>Normalizes each row of a matrix to have unit norm.</p>"},{"location":"spectral-api-reference/#parameters_11","title":"Parameters:","text":"<ul> <li><code>$X</code>: A matrix (such as the spectral embeddings).</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_11","title":"Returns:","text":"<ul> <li>The row-normalized version of <code>$X</code>.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringcluster","title":"<code>spectral-clustering.cluster</code>","text":"<p>Clusters the spectral embeddings using the k-means algorithm.</p>"},{"location":"spectral-api-reference/#parameters_12","title":"Parameters:","text":"<ul> <li><code>$X</code>: The original dataset.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$rbf-kernel-sigma</code>: The sigma parameter for constructing the RBF affinity matrix.</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_12","title":"Returns:","text":"<ul> <li>The centroids obtained after clustering the row-normalized spectral embeddings with k-means.</li> <li>Type: <code>(NPArray ($K $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringfit","title":"<code>spectral-clustering.fit</code>","text":"<p>Performs the full spectral clustering process on the dataset.</p>"},{"location":"spectral-api-reference/#parameters_13","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$rbf-kernel-sigma</code>: The sigma parameter for the RBF kernel (default example: <code>0.1</code>).</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm (default example: <code>100</code>).</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_13","title":"Returns:","text":"<ul> <li>The tuple containing spectral embeddings and the final centroids computed from clustering the spectral embeddings.</li> <li>Type: <code>((NPArray ($N $C)) (NPArray ($K $C)))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringpredict","title":"<code>spectral-clustering.predict</code>","text":"<p>Predicts the cluster assignment for each data point in the dataset using the spectral embeddings and computed centroids.</p>"},{"location":"spectral-api-reference/#parameters_14","title":"Parameters:","text":"<ul> <li>A tuple <code>($embeddings $centroids)</code>, where:</li> <li><code>$embeddings</code>: The spectral embedding matrix.</li> <li><code>$centroids</code>: The centroids obtained from the clustering step.</li> <li>Type: <code>((NPArray ($N $C)) (NPArray ($K $C)))</code></li> <li><code>$num-clusters</code>: The number of clusters.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_14","title":"Returns:","text":"<ul> <li>A vector of cluster labels, one for each data point, determined by assigning each point to the nearest centroid.</li> <li>Type: <code>((NPArray ($N)))</code></li> </ul>"},{"location":"spectral-api-reference/#usage","title":"Usage","text":"<p>To perform spectral clustering on a dataset <code>S</code> (an <code>NPArray</code> of shape <code>(n, d)</code>) with a specified number of clusters (e.g., 3), RBF kernel sigma (e.g., <code>0.1</code>), and a maximum of 100 iterations for k-means, you can use: <pre><code>(=\n    (embeddings-and-centroids)\n    (spectral-clustering.fit (S) 3 0.1 100)\n)\n</code></pre> To predict the cluster labels for the dataset <code>S</code>, use: <pre><code>(=\n    (cluster-labels)\n    (spectral-clustering.predict\n        (embeddings-and-centroids)        \n        3\n    )    \n)\n</code></pre></p>"},{"location":"spectral-method/","title":"Spectral Clustering","text":"<p>https://en.wikipedia.org/wiki/Spectral_clustering</p> <p>The spectral clustering algorithm reduces the dimensionality of the input data by leveraging  the eigenvalues and eigenvectors derived from the data's similarity matrix, and then applies  standard clustering methods, such as k-means, on the reduced data to identify clusters.</p> <p>Specifically, the algorithm constructs a graph structure using the similarity matrix of the data set,  where each entry represents the similarity score between the corresponding pair of data points.  It then employs the eigenvectors associated to the first k eigenvalues of the  normalized graph Laplacian as input features for the k-means algorithm.</p> <p>The data set is represented by the matrix X with shape [N, D], where N is the number of  data points and D is the dimension of the input space. Our goal is to partition the data points into K clusters. Below is a clear, step-by-step outline of the basic spectral clustering algorithm: 1. Compute the similarity matrix W given the input data X. The similarity matrix W is a symmetric matrix with the shape [N, N], where W[i, j] stores the similarity between data points X[i] and X[j]. A Gaussian (RBF) kernel is used for computing the similarity of data points X[i] and X[j], i.e.,  W[i, j] = exp(-|| X[i] - X[j] ||^2 / (2 * sigma^2)), where sigma is the scale parameter of the  Gaussian kernel. 2. Using the similarity matrix, compute the normalized graph Laplacian L_norm: L_norm = I - D^(-1/2) W D^(-1/2), where I is the identity matrix of shape [N, N], and D is the  degree matrix with D[i, i] = \\sum_j W[i, j]. 3. Compute the spectral embeddings U as the eigenvectors of the k smallest eigenvalues of L. The spectral embeddings matrix U has the shape [N, K].  4. Normalize the rows of U to have the sum equal to one. 5. Apply the k-means clustering algorithm to the spectral embeddings matrix U to find the K clusters.</p>"},{"location":"spectral-technical/","title":"Spectral Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 24, 2025 Version: 1.0  </p>"},{"location":"spectral-technical/#abstract","title":"Abstract","text":"<p>This report presents an open\u2011source implementation of spectral clustering in the MeTTa language. By leveraging MeTTa's declarative approach and NumPy bindings, we express graph\u2011based clustering via eigendecomposition of the Laplacian matrix. Our implementation constructs similarity graphs with RBF kernels, computes normalized Laplacians, and extracts low\u2011dimensional embeddings for k\u2011means partitioning. Benchmarks show competitive clustering quality with runtimes of ~1.1s versus scikit\u2011learn's ~0.05s, demonstrating MeTTa's capability to express complex mathematical algorithms while highlighting areas for optimization.</p>"},{"location":"spectral-technical/#introduction","title":"Introduction","text":"<p>Spectral clustering transforms data clustering into a graph partitioning problem, offering advantages over traditional methods for complex, non\u2011convex cluster shapes. Implementing spectral clustering in MeTTa:</p> <ul> <li>Demonstrates MeTTa's capability for linear algebra and eigendecomposition.  </li> <li>Extends MeTTa's ML toolkit with a graph\u2011based clustering method.  </li> <li>Provides a foundation for future spectral methods in MeTTa.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative and functional computations over knowledge metagraphs. See http://www.metta\u2011lang.dev for details.</p>"},{"location":"spectral-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>Spectral clustering operates on the principle that cluster structure can be revealed through the spectrum (eigenvalues and eigenvectors) of a graph Laplacian derived from data similarities.</p> <p>Key steps:</p> <ol> <li> <p>Similarity Graph Construction: Create an affinity matrix $W$ using a Gaussian kernel    <pre><code>W_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n</code></pre></p> </li> <li> <p>Laplacian Matrix: Compute the normalized Laplacian $L_{sym}$    <pre><code>L_{sym} = I - D^{-1/2}WD^{-1/2}\n</code></pre>    where $D$ is the diagonal degree matrix with $D_{ii} = \\sum_j W_{ij}$</p> </li> <li> <p>Spectral Embedding: Find the $k$ eigenvectors corresponding to the smallest eigenvalues of $L_{sym}$, forming a low\u2011dimensional embedding space.</p> </li> <li> <p>Clustering: Apply k\u2011means to the normalized rows of the eigenvector matrix.</p> </li> </ol> <p>The algorithm exploits spectral graph theory: the multiplicity of the eigenvalue 0 equals the number of connected components, and the corresponding eigenvectors encode component membership.</p>"},{"location":"spectral-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa implementation utilizes NumPy bindings for matrix operations and eigendecomposition, with clear separation of algorithmic steps:</p> <ol> <li>Affinity Matrix Construction    First computes squared norms and pairwise distances, then applies RBF kernel:    <pre><code>(spectral-clustering.rbf-affinity-matrix $sqr-distance-matrix-X $rbf-kernel-sigma)\n</code></pre></li> <li>Time Complexity: O(n\u00b2\u00b7d) for distance matrix, O(n\u00b2) for kernel application.  </li> <li> <p>Space Complexity: O(n\u00b2) for the affinity matrix.</p> </li> <li> <p>Degree Matrix and Laplacian    Computes node degrees by summing affinities, then builds the normalized Laplacian:    <pre><code>(spectral-clustering.normalized-laplacian $W $inverse-degree-matrix-W)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b2) for degree calculation, O(n\u00b3) for Laplacian normalization with matrix multiplication.  </li> <li> <p>Space Complexity: O(n\u00b2) for the Laplacian.</p> </li> <li> <p>Eigendecomposition    Extracts eigenvalues and eigenvectors of the Laplacian:    <pre><code>(spectral-clustering.eigh $X)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b3) for full eigendecomposition.  </li> <li> <p>Space Complexity: O(n\u00b2) for eigenvectors.</p> </li> <li> <p>Spectral Embedding    Selects and normalizes eigenvectors for the k smallest eigenvalues:    <pre><code>(spectral-clustering.row-normalize \n  (spectral-clustering.spectral-embeddings $eigh-I $k))\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k) for selection and normalization.  </li> <li> <p>Space Complexity: O(n\u00b7k) for the embedding.</p> </li> <li> <p>Clustering    Applies k\u2011means to the normalized embeddings:    <pre><code>(kmeans.fit $embeddings $num-clusters $max-kmeans-iter 0.0001)\n</code></pre></p> </li> <li>Time Complexity: O(t\u00b7n\u00b7k\u00b2) where t is iterations of k\u2011means.  </li> <li> <p>Space Complexity: O(n\u00b7k) for assignments.</p> </li> <li> <p>Pipeline Integration    The complete pipeline combines these steps in a nested let\u2011expression structure:    <pre><code>(spectral-clustering.fit $X $num-clusters $rbf-kernel-sigma $max-kmeans-iter)\n</code></pre></p> </li> <li>Overall Time Complexity: O(n\u00b3) dominated by eigendecomposition.  </li> <li> <p>Overall Space Complexity: O(n\u00b2) dominated by the affinity and Laplacian matrices.</p> </li> <li> <p>Prediction    Assigns new data to clusters based on nearest centroids:    <pre><code>(spectral-clustering.predict ($embeddings $centroids) $num-clusters)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k) for assignment calculation.  </li> <li>Space Complexity: O(n\u00b7k) for assignment matrix.</li> </ol> <p>The implementation handles the key algorithm steps declaratively, with eigendecomposition being the computational bottleneck for large datasets.</p>"},{"location":"spectral-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn's <code>SpectralClustering</code> offers more options and optimizations for larger datasets:</p> Feature MeTTa Implementation scikit\u2011learn Affinity methods RBF kernel only RBF, nearest neighbors, precomputed Laplacian types normalized symmetric normalized, unnormalized, random walk Eigensolvers full decomposition via NumPy ARPACK, LOBPCG, AMG (sparse options) Scaling techniques none Nystr\u00f6m method for large datasets Final clustering k\u2011means only k\u2011means or discretization Implementation declarative with NumPy bindings optimized C/Cython with specialized solvers <p>The MeTTa implementation provides core functionality while scikit\u2011learn offers additional options for scalability and customization.</p>"},{"location":"spectral-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each, synthetic generation via scikit\u2011learn):</li> <li><code>noisy_moons</code>: Two interleaving half\u2011circles</li> <li><code>varied</code>: Varied variance blobs</li> <li><code>no_structure</code>: Random noise</li> <li>Environment: CPU = 4 cores @ 3.0 GHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1</li> </ul>"},{"location":"spectral-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset Time (s) Silhouette Calinski-Harabasz Davies-Bouldin ARI NMI AMI no-structure 0.69 0.370 359.72 0.863 0 0 0 blobs 1.57 0.457 268.15 0.594 0.568 0.729 0.728 noisy_circles 1.18 0.114 0.0077 240.59 1.000 1.000 1.000 noisy_moon 0.73 0.385 429.54 1.028 1.000 1.000 1.000 varied 0.76 0.627 1474.97 0.642 0.843 0.828 0.827"},{"location":"spectral-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul) \n(import! &amp;self spectral-clustering)\n\n(let \n    $labels \n    (spectral-clustering.predict $fit-outputs 2)\n    (println! $labels)\n)\n</code></pre> <p>This snippet: 1. Imports required modules 2. Fits spectral clustering with <code>k=2</code> clusters 3. Predicts cluster assignments 4. Prints the resulting labels</p>"},{"location":"spectral-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Scalability: The O(n\u00b2) memory requirement for the affinity matrix and O(n\u00b3) time complexity for eigendecomposition limit scaling to large datasets.</li> <li>Affinity options: Currently only supports RBF kernel; could add nearest\u2011neighbor and custom affinities.</li> <li>Eigensolver efficiency: Could integrate specialized solvers like ARPACK for large sparse matrices.</li> <li>Hyperparameter selection: Automatic sigma estimation for the RBF kernel would improve usability.</li> <li>Alternative Laplacians: Add unnormalized and random walk variants.</li> </ul>"},{"location":"spectral-technical/#conclusion","title":"Conclusion","text":"<p>The MeTTa implementation of spectral clustering demonstrates the language's capability to express complex numerical algorithms declaratively. Despite performance differences compared to optimized C/Cython code, the implementation achieves excellent clustering quality on non\u2011convex datasets. The clear functional decomposition allows for future extensions and optimizations while serving as a reference for spectral methods in MeTTa.</p>"},{"location":"spectral-technical/#references","title":"References","text":"<ol> <li>Ng, Jordan, Weiss (2002). On Spectral Clustering: Analysis and an Algorithm.</li> <li>von Luxburg (2007). A Tutorial on Spectral Clustering.</li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.</li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"visualization/","title":"Virtualization in <code>metta_ul</code>: Grounding Pandas, Matplotlib, and Dimensionality Reduction Libraries","text":""},{"location":"visualization/#overview","title":"Overview","text":"<p>The goal of <code>metta_ul</code> is to ground essential Python libraries inside MeTTa, enabling symbolic code to leverage powerful data science tools:</p> <ul> <li>Pandas for data manipulation</li> <li>Matplotlib for plotting</li> <li>Scikit-learn for machine learning, including dimensionality reduction</li> </ul> <p>We started by grounding core functions explicitly but later switched to a general import mechanism with <code>ul-import</code> and <code>ul-from</code> macros, allowing dynamic binding of Python modules and their functions.</p>"},{"location":"visualization/#grounding-approach","title":"Grounding Approach","text":""},{"location":"visualization/#explicit-grounding-vs-general-import-macros","title":"Explicit Grounding vs General Import Macros","text":"<ul> <li>Explicit Grounding: Ground individual functions or classes one by one (e.g., <code>pd.read_csv</code>, <code>plt.plot</code>).</li> <li>General Import Macros: Use <code>ul-import</code> and <code>ul-from</code> to import modules or functions dynamically, providing flexibility and scalability.</li> </ul>"},{"location":"visualization/#import-macros","title":"Import Macros","text":""},{"location":"visualization/#ul-import","title":"<code>ul-import</code>","text":"<p>Import whole modules or assign aliases:</p> <pre><code>! (ul-import pandas as pd)\n! (ul-import matplotlib.pyplot as plt)\n! (ul-import sklearn.decomposition as decomposition)\n! (ul-import sklearn.manifold as manifold)\n</code></pre>"},{"location":"visualization/#ul-from","title":"<code>ul-from</code>","text":"<p>Import specific functions or classes:</p> <pre><code>! (ul-from sklearn.decomposition import PCA)\n! (ul-from sklearn.manifold import TSNE)\n! (ul-from pandas import DataFrame)\n</code></pre>"},{"location":"visualization/#example-usage","title":"Example Usage","text":""},{"location":"visualization/#pandas-import-and-export-csv","title":"Pandas: Import and Export CSV","text":"<pre><code>! (ul-import pandas as pd)\n\n(= (load-csv $filename) (pd.read_csv $filename))\n(= (save-csv $df $filename) (pd.DataFrame.to_csv $df $filename))\n</code></pre>"},{"location":"visualization/#matplotlib-plotting-line-chart","title":"Matplotlib: Plotting Line Chart","text":"<pre><code>! (ul-import matplotlib.pyplot as plt)\n\n(= (plot-line $x $y)\n  (plt.plot $x $y)\n  (plt.show))\n</code></pre>"},{"location":"visualization/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Use scikit-learn's PCA and t-SNE for reducing data dimensionality:</p> <pre><code>! (ul-from sklearn.decomposition import PCA)\n! (ul-from sklearn.manifold import TSNE)\n! (ul-import numpy as np)\n\n(= (reduce-pca $data $components)\n  (let* (\n    ($pca (PCA n_components $components))\n    ($reduced (pca.fit_transform $data))\n  )\n  $reduced))\n\n(= (reduce-tsne $data $components)\n  (let* (\n    ($tsne (TSNE n_components $components))\n    ($reduced (tsne.fit_transform $data))\n  )\n  $reduced)\n)\n</code></pre>"},{"location":"visualization/#summary","title":"Summary","text":"<ul> <li>Virtualization in <code>metta_ul</code> grounds important Python libraries into MeTTa.</li> <li><code>ul-import</code> and <code>ul-from</code> macros dynamically bind Python modules/functions, enabling concise and extensible code.</li> <li>This allows integration of data science workflows \u2014 data loading (pandas), visualization (matplotlib), and machine learning (scikit-learn) \u2014 directly in MeTTa programs.</li> </ul>"}]}