{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"metta-ul: Clustering Algorithms in MeTTa","text":""},{"location":"#overview","title":"Overview","text":"<p>metta-ul is a basic implementation of clustering algorithms in the MeTTa language. It includes implementations of:</p> <ul> <li>K-Means</li> <li>Gaussian Mixture Models (GMM)</li> <li>Spectral Clustering</li> <li>Hierarchical Clustering</li> </ul> <p>This project is packaged as a Python module and includes a Dockerized environment for running tests using <code>pytest</code>.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Ramin Barati - rekino@gmail.com</li> <li>Amirhossein Nourani Zadeh - amirhossein.nouranizadeh@gmail.com</li> <li>Farhoud - farhoud.m7@gmail.com</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.7 or later</li> <li>Docker</li> <li><code>hyperon</code> &gt;= 0.2.2</li> <li><code>scikit-learn</code></li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"#using-docker","title":"Using Docker","text":"<p>Build and run the containerized environment:</p> <pre><code>docker build . -t metta_ul\n</code></pre>"},{"location":"#running-tests","title":"Running Tests","text":""},{"location":"#running-tests-inside-docker","title":"Running tests inside Docker","text":"<p>You can run tests using the provided <code>Makefile</code>. This will:</p> <ol> <li>Build the Docker image</li> <li>Run tests inside a container</li> <li>Clean up the container after the test run</li> </ol> <p>To execute:</p> <pre><code>make test\n</code></pre> <p>Alternatively, if you want to run pytest directly inside Docker:</p> <pre><code>docker run -it --rm --mount type=bind,src=$(pwd),dst=/app --name metta_ul_run metta_ul pytest -s\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a new branch (<code>feature-branch</code>)</li> <li>Commit changes and push to your branch</li> <li>Submit a pull request</li> </ol>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"agglomerative-api-reference/","title":"<code>metta_ul:cluster:agglomerative</code>","text":""},{"location":"agglomerative-api-reference/#overview","title":"Overview","text":"<p>This module implements agglomerative clustering using a priority heap and a union-find structure to efficiently merge clusters until a desired number of clusters remain. It supports multiple linkage strategies: <code>\"single\"</code>, <code>\"complete\"</code>, and <code>\"average\"</code>.</p>"},{"location":"agglomerative-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"agglomerative-api-reference/#agglomerativedistance-matrix","title":"<code>agglomerative.distance-matrix</code>","text":"<p>Computes the pairwise Euclidean distance matrix for the dataset.</p>"},{"location":"agglomerative-api-reference/#parameters","title":"Parameters:","text":"<ul> <li> <p><code>$X</code>: Input dataset as a 2D NumPy array.</p> </li> <li> <p>Type: <code>(NPArray ($n $d))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns","title":"Returns:","text":"<ul> <li> <p>Pairwise distance matrix between points in <code>$X</code>.</p> </li> <li> <p>Type: <code>(NPArray ($n $n))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativelinkage-distance","title":"<code>agglomerative.linkage-distance</code>","text":"<p>Calculates the distance between two clusters based on a specified linkage method.</p>"},{"location":"agglomerative-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li> <p><code>$distance-matrix</code>: Precomputed pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($n $n))</code></p> </li> <li> <p><code>$cluster1</code>: Indices of the first cluster.</p> </li> <li> <p>Type: <code>(NPArray ($a))</code></p> </li> <li> <p><code>$cluster2</code>: Indices of the second cluster.</p> </li> <li> <p>Type: <code>(NPArray ($b))</code></p> </li> <li> <p><code>$linkage</code>: Linkage method (<code>\"single\"</code>, <code>\"complete\"</code>, or <code>\"average\"</code>).</p> </li> <li> <p>Type: <code>String</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_1","title":"Returns:","text":"<ul> <li> <p>Scalar distance between the two clusters.</p> </li> <li> <p>Type: <code>Number</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativeheapify","title":"<code>agglomerative.heapify</code>","text":"<p>Initializes a heap with the smallest pairwise distances between data points.</p>"},{"location":"agglomerative-api-reference/#overload-1","title":"Overload 1:","text":""},{"location":"agglomerative-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li> <p><code>$distance-matrix</code>: Pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($a $a))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_2","title":"Returns:","text":"<ul> <li> <p>Initialized heap of distances.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#overload-2-internal-recursion","title":"Overload 2 (internal recursion):","text":""},{"location":"agglomerative-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li> <p><code>$distance-matrix</code>: Pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($a $a))</code></p> </li> <li> <p><code>$nn</code>: Array of closest neighbor indices.</p> </li> <li> <p>Type: <code>(NPArray ($a))</code></p> </li> <li> <p><code>$i</code>: Index for recursion.</p> </li> <li> <p>Type: <code>Number</code></p> </li> <li> <p><code>$heap</code>: Current heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_3","title":"Returns:","text":"<ul> <li> <p>Updated heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativeheappush","title":"<code>agglomerative.heappush</code>","text":"<p>Pushes new distance entries into the heap involving a newly formed cluster.</p>"},{"location":"agglomerative-api-reference/#overload-1_1","title":"Overload 1:","text":""},{"location":"agglomerative-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li> <p><code>$heap</code>: Current heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> <li> <p><code>$distance-matrix</code>: Pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($a $a))</code></p> </li> <li> <p><code>$linkage</code>: Linkage method.</p> </li> <li> <p>Type: <code>String</code></p> </li> <li> <p><code>$uf</code>: UnionFind data structure.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> <li> <p><code>$new-root</code>: Root of the newly created cluster.</p> </li> <li> <p>Type: <code>Number</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_4","title":"Returns:","text":"<ul> <li> <p>Updated heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#overload-2-internal-recursion_1","title":"Overload 2 (internal recursion):","text":""},{"location":"agglomerative-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li> <p><code>$heap</code>: Current heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> <li> <p><code>$distance-matrix</code>: Pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($a $a))</code></p> </li> <li> <p><code>$linkage</code>: Linkage method.</p> </li> <li> <p>Type: <code>String</code></p> </li> <li> <p><code>$uf</code>: UnionFind structure.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> <li> <p><code>$new-root</code>: Root of new cluster.</p> </li> <li> <p>Type: <code>Number</code></p> </li> <li> <p><code>$roots</code>: All current roots.</p> </li> <li> <p>Type: <code>(NPArray ($k))</code></p> </li> <li> <p><code>$k</code>: Index for recursion.</p> </li> <li> <p>Type: <code>Number</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_5","title":"Returns:","text":"<ul> <li> <p>Updated heap.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativerecursion","title":"<code>agglomerative.recursion</code>","text":"<p>Recursively merges clusters until only <code>$k</code> clusters remain.</p>"},{"location":"agglomerative-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li> <p><code>($parent $count)</code>: UnionFind structure holding cluster merges and count of current clusters.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> <li> <p><code>$distance-matrix</code>: Pairwise distance matrix.</p> </li> <li> <p>Type: <code>(NPArray ($n $n))</code></p> </li> <li> <p><code>$heap</code>: Min-heap of cluster distances.</p> </li> <li> <p>Type: <code>(Heap (Number Number Number))</code></p> </li> <li> <p><code>$linkage</code>: Linkage strategy to use.</p> </li> <li> <p>Type: <code>String</code></p> </li> <li> <p><code>$k</code>: Desired number of clusters.</p> </li> <li> <p>Type: <code>Number</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_6","title":"Returns:","text":"<ul> <li> <p>Updated UnionFind structure with merged clusters.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativecluster","title":"<code>agglomerative.cluster</code>","text":"<p>Performs the full agglomerative clustering procedure using a priority heap and union-find.</p>"},{"location":"agglomerative-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li> <p><code>$X</code>: Input data points.</p> </li> <li> <p>Type: <code>(NPArray ($n $d))</code></p> </li> <li> <p><code>$k</code>: Number of clusters to return.</p> </li> <li> <p>Type: <code>Number</code></p> </li> <li> <p><code>$linkage</code>: Linkage method to use.</p> </li> <li> <p>Type: <code>String</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_7","title":"Returns:","text":"<ul> <li> <p>Final UnionFind structure after merging.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativeassign","title":"<code>agglomerative.assign</code>","text":"<p>Assigns cluster indices to data points based on the final UnionFind structure.</p>"},{"location":"agglomerative-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li> <p><code>$uf</code>: Final UnionFind structure.</p> </li> <li> <p>Type: <code>UnionFind</code></p> </li> <li> <p><code>$assignment</code>: Array for storing assignment results.</p> </li> <li> <p>Type: <code>(NPArray ($n))</code></p> </li> <li> <p><code>$roots</code>: List of cluster roots.</p> </li> <li> <p>Type: <code>(NPArray ($k))</code></p> </li> <li> <p><code>$index</code>: Current index of the root being assigned.</p> </li> <li> <p>Type: <code>Number</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_8","title":"Returns:","text":"<ul> <li> <p>Final cluster assignment array.</p> </li> <li> <p>Type: <code>(NPArray ($n))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#agglomerativefit-predict","title":"<code>agglomerative.fit-predict</code>","text":"<p>Clusters a dataset and returns an array of cluster assignments.</p>"},{"location":"agglomerative-api-reference/#parameters_9","title":"Parameters:","text":"<ul> <li> <p><code>$X</code>: Dataset of shape <code>(n, d)</code>.</p> </li> <li> <p>Type: <code>(NPArray ($n $d))</code></p> </li> <li> <p><code>$k</code>: Number of clusters to form.</p> </li> <li> <p>Type: <code>Number</code></p> </li> <li> <p><code>$linkage</code>: Linkage method (<code>\"single\"</code>, <code>\"complete\"</code>, <code>\"average\"</code>).</p> </li> <li> <p>Type: <code>String</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#returns_9","title":"Returns:","text":"<ul> <li> <p>Cluster assignment for each point.</p> </li> <li> <p>Type: <code>(NPArray ($n))</code></p> </li> </ul>"},{"location":"agglomerative-api-reference/#usage","title":"Usage","text":"<p>To cluster dataset <code>S</code> into 4 clusters using <code>\"average\"</code> linkage:</p> <pre><code>(=\n    (assignments)\n    (agglomerative.fit-predict S 4 \"average\")\n)\n</code></pre>"},{"location":"agglomerative-api-reference/#notes","title":"Notes","text":"<ul> <li>This implementation is optimized using a heap for fast retrieval of the smallest cluster distances and a union-find structure to avoid merging already connected clusters.</li> <li>All distance computations are based on the Euclidean norm.</li> <li>The use of <code>np.ix_</code> allows efficient distance slicing between cluster indices.</li> <li>The recursive definitions are tail-optimized for stackless processing.</li> </ul>"},{"location":"agglomerative-technical/","title":"Agglomerative Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh</p> <p>Date: July 29, 2025</p> <p>Version: 1.0</p>"},{"location":"agglomerative-technical/#abstract","title":"Abstract","text":"<p>We present an implementation of agglomerative (hierarchical) clustering in the MeTTa programming language, incorporating a min-heap and union-find data structures for performance optimization. By avoiding repeated full pairwise searches and tracking cluster memberships with union-find, the algorithm achieves significantly improved performance over naive approaches. This report details the algorithm's design, MeTTa-specific implementations, and performance characteristics.</p>"},{"location":"agglomerative-technical/#introduction","title":"Introduction","text":"<p>Hierarchical clustering is widely used for grouping unlabeled data without requiring a priori knowledge of the number of clusters. Agglomerative clustering performs this by repeatedly merging the closest pair of clusters until the desired number of clusters is reached.</p> <p>This MeTTa implementation is designed to:</p> <ul> <li>Demonstrate efficient recursive programming with MeTTa.</li> <li>Serve as a high-performance clustering module for symbolic and numerical data.</li> <li>Showcase how classical data structures (heap, union-find) can be expressed declaratively.</li> </ul>"},{"location":"agglomerative-technical/#algorithm-overview","title":"Algorithm Overview","text":"<ol> <li>Distance Matrix Computation: Compute full pairwise distances between data points.</li> <li>Heap Initialization: Build a min-heap with the smallest distance for each point.</li> <li>Union-Find Initialization: Each point begins in its own set.</li> <li>Recursive Merge: Pop the heap to find the nearest cluster pair. If not already unified, merge them and update the heap with distances to the new cluster.</li> <li>Label Assignment: Assign a unique label to each disjoint set.</li> </ol>"},{"location":"agglomerative-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":""},{"location":"agglomerative-technical/#1-distance-matrix-agglomerativedistance-matrix","title":"1. Distance Matrix (<code>agglomerative.distance-matrix</code>)","text":"<p>Computes an \\$n \\times n\\$ matrix of pairwise Euclidean distances.</p> <ul> <li>Implementation: Uses broadcasting via <code>np.expand_dims</code> and <code>np.linalg.norm</code>.</li> <li>Time: O(n\u00b2\u00b7d)</li> <li>Memory: O(n\u00b2)</li> </ul>"},{"location":"agglomerative-technical/#2-linkage-function-agglomerativelinkage-distance","title":"2. Linkage Function (<code>agglomerative.linkage-distance</code>)","text":"<p>Computes distance between two clusters using a linkage criterion:</p> <ul> <li> <p>single: minimum pairwise distance</p> </li> <li> <p>complete: maximum pairwise distance</p> </li> <li> <p>average: mean pairwise distance</p> </li> <li> <p>Time: O(|C\u2081|\u00b7|C\u2082|)</p> </li> <li> <p>Memory: O(|C\u2081|\u00b7|C\u2082|)</p> </li> </ul>"},{"location":"agglomerative-technical/#3-heap-initialization-agglomerativeheapify","title":"3. Heap Initialization (<code>agglomerative.heapify</code>)","text":"<p>For each point, identifies its nearest neighbor and inserts the pair with their distance into a min-heap.</p> <ul> <li>Implementation: Uses masked distance matrix to avoid self-pairing.</li> <li>Time: O(n\u00b2)</li> <li>Memory: O(n)</li> </ul>"},{"location":"agglomerative-technical/#4-heap-update-agglomerativeheappush","title":"4. Heap Update (<code>agglomerative.heappush</code>)","text":"<p>After a merge, computes the distance between the new cluster and all remaining clusters and pushes these distances to the heap.</p> <ul> <li>Implementation: Recursively compares new root with others using union-find and linkage function.</li> <li>Time: O(n\u00b7p) where p = cost of linkage calculation</li> <li>Memory: Heap contains up to O(n\u00b2) entries over time.</li> </ul>"},{"location":"agglomerative-technical/#5-union-find-recursion-agglomerativerecursion","title":"5. Union-Find Recursion (<code>agglomerative.recursion</code>)","text":"<p>Recursive loop merges the closest cluster pair at each step, skipping pairs already unified.</p> <ul> <li> <p>Implementation:</p> </li> <li> <p>Uses <code>UnionFind.areUnified</code> to avoid redundant merges.</p> </li> <li>After each merge, updates heap with new distances.</li> <li>Time: Worst-case O((n\u2013k)\u00b7n\u00b2\u00b7p), where p is linkage cost</li> <li>Memory: Tail-recursive stack, union-find trees + heap</li> </ul>"},{"location":"agglomerative-technical/#6-cluster-routine-agglomerativecluster","title":"6. Cluster Routine (<code>agglomerative.cluster</code>)","text":"<p>Main pipeline for clustering: computes distances, initializes heap and union-find, runs merge recursion.</p> <ul> <li>Time: Aggregate of above steps</li> <li>Memory: Aggregate of above steps</li> </ul>"},{"location":"agglomerative-technical/#7-cluster-assignment-agglomerativeassign","title":"7. Cluster Assignment (<code>agglomerative.assign</code>)","text":"<p>Labels each element according to the root of its union-find group.</p> <ul> <li>Time: O(n)</li> <li>Memory: O(n)</li> </ul>"},{"location":"agglomerative-technical/#8-api-entry-point-agglomerativefit-predict","title":"8. API Entry Point (<code>agglomerative.fit-predict</code>)","text":"<p>Wrapper that runs clustering and returns an <code>np.array</code> of cluster labels.</p> <ul> <li>Time: Total time from above steps</li> <li>Memory: O(n\u00b2)</li> </ul>"},{"location":"agglomerative-technical/#comparison-with-naive-agglomerative-eg-scikit-learn","title":"Comparison with Naive Agglomerative (e.g., scikit-learn)","text":"Feature MeTTa Optimized Implementation scikit\u2011learn Linkage types single, complete, average ward, complete, average, single Distance storage full matrix (numpy) condensed or sparse matrix Search strategy heap-based nearest-neighbor tracking fast C loops, optional connectivity Merge bookkeeping union-find tree / linkage matrix Parallelization not yet supported not supported in <code>AgglomerativeClustering</code> Best-case runtime O(n\u00b2\u00b7log\u202fn) with fast linkage O(n\u00b2) for complete linkage"},{"location":"agglomerative-technical/#benchmark-setup","title":"Benchmark Setup","text":"<p>The allgorithm does not return in an acceptable time even for pretty small datasets (~20 samples).</p>"},{"location":"agglomerative-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul:cluster:agglomerative)\n\n;; Run average-linkage clustering with k = 3\n(let $labels (agglomerative.fit-predict $X 3 \"average\")\n    (println! $labels)\n)\n</code></pre> <p>Where <code>$X</code> is an <code>np.array</code> of shape <code>(n, d)</code>.</p>"},{"location":"agglomerative-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Dense Distance Matrix: O(n\u00b2) memory use limits scalability.</li> <li>Heap Growth: No deduplication; heap may grow large unless compacted.</li> <li>Condensed Matrix Support: Could reduce memory footprint with careful index mapping.</li> </ul>"},{"location":"agglomerative-technical/#planned-extensions","title":"Planned Extensions:","text":"<ul> <li>Add support for sparse graph connectivity (e.g., radius or k-NN graphs).</li> <li>Use priority queue with distance caching to avoid redundant linkage evaluations.</li> </ul>"},{"location":"agglomerative-technical/#conclusion","title":"Conclusion","text":"<p>This report documents a fast, modular, and expressive implementation of agglomerative clustering in MeTTa. The use of declarative recursion, heaps, and union-find enables realistic performance on medium-scale datasets and lays the groundwork for extending MeTTa into more advanced unsupervised learning domains.</p>"},{"location":"agglomerative-technical/#references","title":"References","text":"<ol> <li>Johnson, S. C. (1967). Hierarchical clustering schemes.</li> <li>Pedregosa et al. (2011). Scikit-learn: Machine learning in Python.</li> <li>MeTTa Language Specification. http://www.metta-lang.dev/spec</li> <li>Tarjan, R. E. (1975). Efficiency of a good but not linear set union algorithm.</li> </ol> <p>Would you like me to generate:</p> <ul> <li>a visual diagram of the algorithm flow?</li> <li>benchmark plots?</li> <li>unit test examples in MeTTa?</li> <li>a compact README.md version of this?</li> </ul> <p>Let me know what you need next.</p>"},{"location":"bisecting-kmeans-api-reference/","title":"metta_ul:cluster:bisecting-kmeans","text":""},{"location":"bisecting-kmeans-api-reference/#overview","title":"Overview","text":"<p>This module implements the Bisecting K-means clustering algorithm using iterative splitting of clusters. The algorithm starts with the entire dataset as a single cluster and recursively bisects the cluster with the maximum Sum of Squared Errors (SSE) using standard k-means (with k=2) until a desired number of clusters is reached. The resulting hierarchical clustering structure captures the splits performed during the process.</p>"},{"location":"bisecting-kmeans-api-reference/#type-definitions","title":"Type Definitions","text":""},{"location":"bisecting-kmeans-api-reference/#core-types","title":"Core Types","text":"<ul> <li><code>Cluster</code>: A tuple <code>($indices $center $sse $hierarchy)</code> representing a single cluster</li> <li><code>(List Cluster)</code>: A typed list of clusters using <code>Cons</code> and <code>Nil</code> constructors</li> <li><code>(List (List Cluster))</code>: Represents the hierarchical clustering structure</li> </ul>"},{"location":"bisecting-kmeans-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscompute-sse","title":"<code>bisecting-kmeans.compute-sse</code>","text":"<p>Computes the Sum of Squared Errors (SSE) for a given cluster.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$indices</code>: Indices of the data points belonging to the cluster.</li> <li>Type: <code>(NPArray ($M))</code></li> <li><code>$centers</code>: The center (mean) of the cluster.</li> <li>Type: <code>(NPArray ($C))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns","title":"Returns:","text":"<ul> <li>The SSE value computed as the sum of squared differences between the data points and the cluster center.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscompute-initial-cluster","title":"<code>bisecting-kmeans.compute-initial-cluster</code>","text":"<p>Computes the initial cluster for the dataset.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A list containing the initial cluster.</li> <li>Type: <code>(List Cluster)</code></li> </ul> <p>Where the <code>(List Cluster)</code> is a list of clusters, and each cluster is a tuple containing: 1. Indices: All data point indices in <code>$X</code>.    - Type: <code>(NPArray ($N $D))</code> 2. Center: The mean of the dataset.    - Type: <code>(NPArray ($C))</code> 3. SSE: The computed SSE for the cluster.    - Type: <code>Number</code> 4. Hierarchy: <code>pyNone</code> (as no hierarchy is present initially).    - Type: <code>(List (List Cluster))</code></p>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-indices","title":"<code>bisecting-kmeans.get-cluster-indices</code>","text":"<p>Extracts the indices from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple <code>($indices $center $sse $hierarchy)</code> representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>The indices of the data points in the cluster.</li> <li>Type: <code>(NPArray ($M))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-center","title":"<code>bisecting-kmeans.get-cluster-center</code>","text":"<p>Extracts the center of the cluster.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>The center of the cluster.</li> <li>Type: <code>(NPArray ($C))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-sse","title":"<code>bisecting-kmeans.get-cluster-sse</code>","text":"<p>Extracts the SSE value from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>The SSE value of the cluster.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansget-cluster-hierarchy","title":"<code>bisecting-kmeans.get-cluster-hierarchy</code>","text":"<p>Extracts the hierarchical structure information from a cluster tuple.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li><code>$cluster</code>: A tuple representing a cluster.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_5","title":"Returns:","text":"<ul> <li>The hierarchy associated with the cluster.</li> <li>Type: <code>(List (List Cluster))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansfind-max-cluster","title":"<code>bisecting-kmeans.find-max-cluster</code>","text":"<p>Finds the cluster with the maximum SSE from a list of clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li>A list of clusters, where each cluster is a tuple <code>($indices $center $sse $hierarchy)</code>.</li> <li>Type: <code>(List Cluster)</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_6","title":"Returns:","text":"<ul> <li>The cluster tuple with the highest SSE value.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanscluster-equal","title":"<code>bisecting-kmeans.cluster-equal</code>","text":"<p>Checks whether two clusters are equal based on their indices, center, and SSE.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li>Two cluster tuples: <code>($indices1 $center1 $sse1 $hierarchy1)</code> and <code>($indices2 $center2 $sse2 $hierarchy2)</code>.</li> <li>Type: <code>Cluster Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_7","title":"Returns:","text":"<ul> <li><code>True</code> if both clusters are equal; otherwise, <code>False</code>.</li> <li>Type: <code>Bool</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansremove-cluster","title":"<code>bisecting-kmeans.remove-cluster</code>","text":"<p>Removes a target cluster from a list of clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li><code>$clusters</code>: The list of current clusters.</li> <li>Type: <code>(List Cluster)</code></li> <li><code>$target</code>: The cluster tuple to be removed.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_8","title":"Returns:","text":"<ul> <li>An updated list of clusters with the target cluster removed.</li> <li>Type: <code>Cluster</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansbisect-cluster","title":"<code>bisecting-kmeans.bisect-cluster</code>","text":"<p>Performs bisection on a given cluster using standard k-means with k=2.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_9","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$max-cluster</code>: The cluster to be bisected, represented as a tuple.</li> <li>Type: <code>Cluster</code></li> <li><code>$max-iter</code>: The maximum number of iterations allowed for the k-means algorithm during the bisecting process.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_9","title":"Returns:","text":"<ul> <li>A tuple containing two clusters obtained from splitting the input cluster. Each cluster is represented as <code>(indices, center, sse, hierarchy)</code>.</li> <li>Type: <code>(List Cluster)</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansrecursive-bisecting-kmeans","title":"<code>bisecting-kmeans.recursive-bisecting-kmeans</code>","text":"<p>Recursively applies bisecting k-means to further split clusters until the desired number of clusters is reached.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_10","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$clusters</code>: The current list of clusters.</li> <li>Type: <code>(List Cluster)</code></li> <li><code>$max-num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$max-iter</code>: The maximum iterations for each bisecting step.</li> <li>Type: <code>Number</code></li> <li><code>$hierarchy</code>: The current hierarchical clustering structure maintained as a MeTTa list.</li> <li>Type: <code>(List (List Cluster))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_10","title":"Returns:","text":"<ul> <li>An updated hierarchical clustering structure as a MeTTa list describing the clustering process.</li> <li>Type: <code>(List (List Cluster))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansfit","title":"<code>bisecting-kmeans.fit</code>","text":"<p>Performs bisecting k-means clustering on the dataset.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_11","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$max-num-clusters</code>: The maximum (desired) number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm during the bisecting process.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_11","title":"Returns:","text":"<ul> <li>A hierarchical clustering structure as a MeTTa list that describes the sequence of splits performed.</li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansassign-point-to-cluster","title":"<code>bisecting-kmeans.assign-point-to-cluster</code>","text":"<p>Assigns a single data point to the closest cluster based on Euclidean distance.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_12","title":"Parameters:","text":"<ul> <li><code>$point</code>: A single data point from the dataset.</li> <li>Type: <code>(NPArray ($D))</code></li> <li><code>$clusters</code>: A list of clusters.</li> <li>Type: <code>(List Cluster)</code></li> <li><code>$best-cluster-idx</code>: The current best cluster index for the point (initial value provided).</li> <li>Type: <code>Number</code></li> <li><code>$best-distance</code>: The current best distance found (initially set to a high value, such as <code>pyINF</code>).</li> <li>Type: <code>Number</code></li> <li><code>$cluster-idx</code>: The current index being evaluated.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_12","title":"Returns:","text":"<ul> <li>The index of the cluster that is closest to the given data point.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeansassign-all-points","title":"<code>bisecting-kmeans.assign-all-points</code>","text":"<p>Assigns all data points in the dataset to their closest clusters.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_13","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($D))</code></li> <li><code>$clusters</code>: The list of clusters.</li> <li>Type: <code>(List Cluster)</code></li> <li><code>$point-idx</code>: The current index of the data point being processed.</li> <li>Type: <code>Number</code></li> <li><code>$labels</code>: A list of cluster labels corresponding to the assignment of each data point.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_13","title":"Returns:","text":"<ul> <li>A list of cluster labels indicating the assignment of each data point in <code>$X</code> to the nearest cluster.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#bisecting-kmeanspredict","title":"<code>bisecting-kmeans.predict</code>","text":"<p>Predicts the cluster membership for each data point based on the final hierarchical clustering structure.</p>"},{"location":"bisecting-kmeans-api-reference/#parameters_14","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$hierarchy</code>: The hierarchical clustering structure generated by <code>bisecting-kmeans.fit</code>.</li> <li>Type: <code>(List (List Cluster))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#returns_14","title":"Returns:","text":"<ul> <li>A list of cluster labels indicating the assigned cluster for each data point in the dataset.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"bisecting-kmeans-api-reference/#usage","title":"Usage","text":"<p>To perform bisecting k-means clustering on a dataset <code>S</code> (an <code>NPArray</code> of shape <code>(n, d)</code>) with a maximum of 5 clusters and 100 maximum iterations for k-means: <pre><code>(=\n    (clustering-history)\n    (bisecting-kmeans.fit (S) 5 100)\n)\n</code></pre> After clustering, you can predict the cluster assignments for S as follows: <pre><code>(=\n    (cluster-labels)\n    (bisecting-kmeans.predict (S) (clustering-history))\n)\n</code></pre></p>"},{"location":"bisecting-kmeans-api-reference/#notes","title":"Notes","text":"<ul> <li> <p>The algorithm repeatedly splits clusters by applying standard k-means with k=2,  targeting the cluster with the highest SSE for bisection.</p> </li> <li> <p>The clustering process maintains a hierarchical structure that records the  splits performed, which can be used for further analysis or visualization.</p> </li> <li> <p>SSE is used as the criterion for selecting the cluster to bisect,  ensuring that the cluster with the largest dispersion is split at each iteration.</p> </li> </ul>"},{"location":"bisecting-kmeans-technical/","title":"Bisecting K-means in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 24, 2025 Version: 1.0  </p>"},{"location":"bisecting-kmeans-technical/#abstract","title":"Abstract","text":"<p>This report presents an implementation of Bisecting K-means clustering in the MeTTa language. Bisecting K-means is a hierarchical variant of the standard K-means algorithm that recursively divides clusters through binary splits. Our implementation constructs a complete divisive clustering solution, maintaining a hierarchical structure of the resulting partitions. Benchmarks show competitive clustering quality with runtimes of ~4.9s, offering a robust hierarchical approach that's conceptually simpler than agglomerative methods. This work demonstrates MeTTa's capability to express recursive algorithms with complex data structures while highlighting both strengths and areas for optimization in hierarchical clustering.</p>"},{"location":"bisecting-kmeans-technical/#introduction","title":"Introduction","text":"<p>Bisecting K-means is a divisive hierarchical clustering approach that offers several advantages over standard K-means and agglomerative methods:</p> <ul> <li>Produces a hierarchical tree structure that represents clustering at multiple levels</li> <li>Generally more efficient than agglomerative approaches for large datasets</li> <li>Often creates more balanced clusters than standard K-means</li> <li>Follows a top-down approach that can be more intuitive for certain applications</li> </ul> <p>Implementing Bisecting K-means in MeTTa:</p> <ul> <li>Demonstrates MeTTa's capability to implement recursive algorithms</li> <li>Showcases MeTTa's ability to represent hierarchical data structures</li> <li>Extends MeTTa's ML toolkit with a hierarchical clustering approach</li> <li>Targets open\u2011source users in AI and Data Science</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative and functional computations over knowledge metagraphs. See http://www.metta\u2011lang.dev for details.</p>"},{"location":"bisecting-kmeans-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>Bisecting K-means operates by recursively splitting clusters until the desired number of clusters is reached. The core strategy follows these steps:</p> <ol> <li>Initialization: Begin with all data points in a single cluster</li> <li>Iterative Bisection:</li> <li>Select the cluster with the largest Sum of Squared Errors (SSE)</li> <li>Apply standard K-means with k=2 to divide this cluster into two subclusters</li> <li>Add the two resulting clusters to the cluster list</li> <li>Remove the original cluster from the list</li> <li>Termination: Continue until the desired number of clusters is reached</li> </ol> <p>The algorithm leverages the divisive hierarchical approach, which starts with all points in one cluster and recursively splits clusters. The key insight is that by always splitting the cluster with the largest SSE, we prioritize improving the clustering quality by focusing on the most heterogeneous clusters first.</p>"},{"location":"bisecting-kmeans-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa implementation organizes clusters as tuples of (indices, center, SSE, hierarchy) and uses recursion for the bisecting process:</p> <ol> <li>Cluster Representation    Each cluster is represented as a tuple containing:    <pre><code>($indices $center $sse $hierarchy)\n</code></pre></li> <li><code>$indices</code>: Array of indices for data points in the cluster</li> <li><code>$center</code>: Centroid of the cluster</li> <li><code>$sse</code>: Sum of squared errors within the cluster</li> <li> <p><code>$hierarchy</code>: Hierarchical structure of subclusters</p> </li> <li> <p>Initial Cluster Creation    Begins with all data points in a single cluster:    <pre><code>(bisecting-kmeans.compute-initial-cluster $X)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7d) for computing the initial mean</li> <li> <p>Space Complexity: O(n) for storing indices</p> </li> <li> <p>Finding Cluster to Split    Selects the cluster with the largest SSE:    <pre><code>(bisecting-kmeans.find-max-cluster $clusters)\n</code></pre></p> </li> <li>Time Complexity: O(k) where k is the current number of clusters</li> <li> <p>Space Complexity: O(1) for storing the max cluster</p> </li> <li> <p>Bisecting a Cluster    Applies K-means with k=2 to split the selected cluster:    <pre><code>(bisecting-kmeans.bisect-cluster $X $max-cluster $max-iter)\n</code></pre></p> </li> <li>Time Complexity: O(t\u00b7n'\u00b7d) where t is K-means iterations and n' is points in the cluster</li> <li> <p>Space Complexity: O(n') for storing the new cluster assignments</p> </li> <li> <p>Recursive Bisecting Process    Recursively applies the bisecting process until reaching the desired number of clusters:    <pre><code>(bisecting-kmeans.recursive-bisecting-kmeans $X $clusters $max-num-clusters $max-iter $hierarchy)\n</code></pre></p> </li> <li>Time Complexity: O(k\u00b7t\u00b7n\u00b7d) where k is the target number of clusters</li> <li> <p>Space Complexity: O(k\u00b7n) for storing all cluster hierarchies</p> </li> <li> <p>Complete Fitting Function    Orchestrates the complete bisecting K-means algorithm:    <pre><code>(bisecting-kmeans.fit $X $max-num-clusters $max-kmeans-iter)\n</code></pre></p> </li> <li>Overall Time Complexity: O(k\u00b7t\u00b7n\u00b7d)</li> <li> <p>Overall Space Complexity: O(k\u00b7n)</p> </li> <li> <p>Prediction Function    Assigns new data points to clusters based on closest centroids:    <pre><code>(bisecting-kmeans.predict $X $hierarchy)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for assignment calculation</li> <li>Space Complexity: O(n) for assignment array</li> </ol> <p>The implementation handles recursive clustering through functional decomposition, with the hierarchical structure explicitly maintained during the bisecting process.</p>"},{"location":"bisecting-kmeans-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn's <code>BisectingKMeans</code> (introduced in version 1.1) offers some optimizations compared to our MeTTa implementation:</p> Feature MeTTa Implementation scikit\u2011learn Cluster selection Largest SSE only Largest SSE or random Bisection method Standard K-means K-means or K-means++ Parallel execution None None (no n_jobs parameter) Hierarchy tracking Full hierarchy maintained Final clusters only Memory efficiency Stores all intermediate clusters More memory-efficient implementation Implementation Recursive with functional approach Iterative with optimized C/Cython <p>The MeTTa implementation provides explicit hierarchy tracking that scikit\u2011learn doesn't expose, while scikit\u2011learn offers better performance optimizations.</p>"},{"location":"bisecting-kmeans-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each, synthetic generation via scikit\u2011learn):</li> <li><code>noisy_circles</code>: Two concentric circles</li> <li><code>noisy_moons</code>: Two interleaving half\u2011circles</li> <li><code>varied</code>: Varied variance blobs</li> <li>Environment: CPU = 4 cores @ 3.0 GHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1</li> </ul>"},{"location":"bisecting-kmeans-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset MeTTa Time (s) scikit\u2011learn Time (s) Silhouette Calinski\u2011Harabasz Davies\u2011Bouldin ARI NMI AMI blobs 3.036 &lt; 0.01 0.6541 1423.97 0.4787 0.9821 0.9691 0.9690 noisy_moon 1.541 &lt; 0.01 0.4956 690.81 0.8120 0.4834 0.3857 0.3848 no_structure 3.244 &lt; 0.01 0.3629 356.97 0.8715 0.0000 0.0000 0.0000 varied 2.996 &lt; 0.01 0.6396 1548.48 0.6096 0.7266 0.7313 0.7303 noisy_circles 1.129 &lt; 0.01 0.3496 282.17 1.1915 \u22120.0019 0.0001 \u22120.0014"},{"location":"bisecting-kmeans-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul) \n(import! &amp;self bisecting-kmeans)\n\n(let \n    $labels \n    (bisecting-kmeans.predict X $hierarchy)\n    (println! $labels)\n)\n</code></pre> <p>This snippet: 1. Imports required modules 2. Fits Bisecting K-means with 2 clusters and max 10 iterations for each K-means application 3. Predicts cluster assignments 4. Prints the resulting labels</p>"},{"location":"bisecting-kmeans-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Non-convex Clusters: As with standard K-means, Bisecting K-means struggles with non-convex cluster shapes, as shown in the benchmark results for <code>noisy_circles</code>.</li> <li>Cluster Selection Strategy: Currently only selects the cluster with largest SSE; could be extended with different selection criteria like random selection or cluster size.</li> <li>Initialization Method: Uses standard K-means for bisection; could benefit from K-means++ initialization.</li> <li>Memory Efficiency: Stores the complete hierarchy which could be memory-intensive for large datasets with many clusters.</li> <li>Termination Criteria: Currently only terminates based on number of clusters; could add additional criteria based on minimum SSE improvement.</li> </ul>"},{"location":"bisecting-kmeans-technical/#conclusion","title":"Conclusion","text":"<p>The MeTTa implementation of Bisecting K-means demonstrates the language's capability to express recursive algorithms and hierarchical data structures. While the algorithm doesn't outperform Spectral Clustering on complex geometries, it provides valuable hierarchical information not available in flat clustering methods. The functional and declarative approach in MeTTa makes the algorithm structure clear and maintainable, though with some performance trade-offs compared to optimized implementations.</p> <p>Bisecting K-means represents an important middle ground between simple partitional methods like K-means and more complex techniques like Spectral Clustering, making it a valuable addition to the MeTTa machine learning toolkit, particularly for applications where hierarchical structure is important and clusters are relatively convex.</p>"},{"location":"bisecting-kmeans-technical/#references","title":"References","text":"<ol> <li>Steinbach, M., Karypis, G., &amp; Kumar, V. (2000). A comparison of document clustering techniques.</li> <li>Savaresi, S. M., &amp; Boley, D. L. (2004). A comparative analysis on the bisecting K-means and the PDDP clustering algorithms.</li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.</li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"gmm-api-reference/","title":"metta_ul:cluster:gmm","text":""},{"location":"gmm-api-reference/#overview","title":"Overview","text":"<p>This module implements the Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm in MeTTa. GMM is a probabilistic model for representing normally distributed subpopulations within an overall dataset.</p>"},{"location":"gmm-api-reference/#functions","title":"Functions","text":""},{"location":"gmm-api-reference/#gmmcenter","title":"<code>gmm.center</code>","text":"<p>Centers the data points by subtracting the means.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code></p> <p>Returns: - Centered data matrix.     - Type: <code>(NPArray ($n $k $d))</code></p>"},{"location":"gmm-api-reference/#gmmmahalanobis-term","title":"<code>gmm.mahalanobis-term</code>","text":"<p>Computes the Mahalanobis distance term for the Gaussian probability density function.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Mahalanobis distance matrix.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmgaussian-pdf","title":"<code>gmm.gaussian-pdf</code>","text":"<p>Computes the probability density function (PDF) for a multivariate Gaussian distribution.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Matrix of Gaussian probabilities for each point and cluster.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmlog-likelihood","title":"<code>gmm.log-likelihood</code>","text":"<p>Computes the log-likelihood of the dataset given the current GMM parameters.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$weights</code>: Mixture component weights.     - Type: <code>(NPArray ($k))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Log-likelihood scalar value.     - Type: <code>Number</code></p>"},{"location":"gmm-api-reference/#gmminit","title":"<code>gmm.init</code>","text":"<p>Initializes the GMM parameters (weights, means, and covariances).</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$k</code>: Number of Gaussian components.</p> <p>Returns: - Initial weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmme-step","title":"<code>gmm.e-step</code>","text":"<p>Performs the Expectation (E) step of the EM algorithm, computing responsibilities.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$weights</code>: Mixture component weights.     - Type: <code>(NPArray ($k))</code> - <code>$means</code>: Mean vectors of the clusters.     - Type: <code>(NPArray ($k $d))</code> - <code>$covariances</code>: Covariance matrices for each cluster.     - Type: <code>(NPArray ($k $d $d))</code></p> <p>Returns: - Responsibility matrix.     - Type: <code>(NPArray ($n $k))</code></p>"},{"location":"gmm-api-reference/#gmmm-step","title":"<code>gmm.m-step</code>","text":"<p>Performs the Maximization (M) step of the EM algorithm, updating parameters.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$responsibilities</code>: Responsibility matrix from the E-step.     - Type: <code>(NPArray ($n $k))</code></p> <p>Returns: - Updated weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmmrecursion","title":"<code>gmm.recursion</code>","text":"<p>Recursively applies the EM steps until the maximum number of iterations is reached.</p> <p>Parameters: - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>($weights, $means, $covariances)</code>: Current GMM parameters.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code> - <code>$max-iter</code>: Maximum number of iterations.     - Type: <code>Number</code></p> <p>Returns: - Final weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#gmm","title":"<code>gmm</code>","text":"<p>Main function to train a GMM on a dataset.</p> <p>Parameters - <code>$X</code>: Data points as a matrix.     - Type: <code>(NPArray ($n $d))</code> - <code>$k</code>: Count of components.     - Type: <code>Number</code> - <code>$max-iter</code>: Maximum number of iterations.     - Type: <code>Number</code></p> <p>Returns: - Final weights, means, and covariance matrices.     - Type: <code>((NPArray ($k)) (NPArray ($k $d)) (NPArray ($k $d $d)))</code></p>"},{"location":"gmm-api-reference/#usage","title":"Usage","text":"<p>To cluster a dataset <code>S</code> of type <code>(NPArray ($n $d))</code> into 3 clusters with default settings: <pre><code>(=\n    (params)\n    (gmm (S) 3)\n)\n</code></pre> To specify a maximum number of 50 iterations: <pre><code>(gmm (S) 3 50)\n</code></pre> To assign a dataset <code>X</code> of type <code>(NPArray ($m $d))</code> using <code>params</code>: <pre><code>(=\n    (assignments)\n    (gmm.e-step (X) (params))\n)\n</code></pre></p>"},{"location":"gmm-api-reference/#notes","title":"Notes","text":"<ul> <li>The GMM parameters are initialized using random means selected from the dataset and a slightly perturbed covariance matrix.</li> <li>The EM algorithm iteratively refines the parameters to maximize the log-likelihood of the data.</li> <li>Responsibilities indicate the probability of each data point belonging to each cluster.</li> </ul>"},{"location":"gmm-api-reference/#dependencies","title":"Dependencies","text":"<p>This module relies on the following NumPy operations within MeTTa: - <code>np.sub</code>: Element-wise subtraction. - <code>np.einsum</code>: Einstein summation notation for efficient matrix operations. - <code>np.linalg.inv</code>: Inversion of covariance matrices. - <code>np.linalg.slogabsdet</code>: Log determinant of covariance matrices. - <code>np.mul</code>: Element-wise multiplication. - <code>np.add</code>: Element-wise addition. - <code>np.sum</code>: Summation along an axis. - <code>np.log</code>: Logarithm function. - <code>np.div</code>: Element-wise division. - <code>np.exp</code>: Exponential function. - <code>np.ones</code>: Creates an array of ones. - <code>np.repeat</code>: Repeats an array along a specified axis. - <code>np.eye</code>: Creates an identity matrix. - <code>np.choose</code>: Selects random initial means from the dataset. - <code>np.cov</code>: Computes the covariance matrix.</p> <p>This implementation provides a full pipeline for training a Gaussian Mixture Model using the EM algorithm in MeTTa.</p>"},{"location":"gmm-technical/","title":"Gaussian Mixture Model (GMM) Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzedeh Date: May 21, 2025 Version: 1.0</p>"},{"location":"gmm-technical/#abstract","title":"Abstract","text":"<p>This report details an open\u2011source implementation of the Gaussian Mixture Model (GMM) clustering algorithm in the MeTTa language. It showcases MeTTa\u2019s declarative and functional strengths by expressing the Expectation\u2013Maximization steps\u2014without native loops\u2014using recursive function definitions and NumPy bindings. Performance considerations and complexity analyses are discussed in the context of future improvements.</p>"},{"location":"gmm-technical/#introduction","title":"Introduction","text":"<p>Probabilistic clustering via GMM models each cluster as a multivariate Gaussian, allowing soft assignments and richer cluster shapes compared to KMeans. Embedding GMM in MeTTa:</p> <ul> <li>Extends MeTTa\u2019s numeric capabilities with probabilistic models.  </li> <li>Provides a template for future statistical algorithm implementations.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative computations over knowledge graphs. See http://www.metta-lang.dev for more information.</p>"},{"location":"gmm-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>GMM uses the Expectation\u2013Maximization (EM) algorithm to maximize data likelihood under a mixture of Gaussians:</p> <ol> <li>E\u2011Step: Compute responsibilities $r_{ik} = p(z_k|x_i)$ via Gaussian PDFs and mixture weights.  </li> <li>M\u2011Step: Update weights $\\pi_k$, means $\\mu_k$, and covariances $\\Sigma_k$ based on responsibilities.  </li> <li>Repeat until convergence or max iterations.</li> </ol> <p>Mathematically:</p> <ul> <li>Gaussian PDF: <pre><code>p_k(x) = \\exp\\bigl(-\\tfrac12 (x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) - \\tfrac{d}{2}\\ln(2\\pi) - \\tfrac12 \\ln|\\Sigma_k|\\bigr)\n</code></pre></li> <li>Log\u2011Likelihood: <pre><code>  L = \\sum_{i=1}^n \\ln \\Bigl( \\sum_{k=1}^K \\pi_k p_k(x_i) \\Bigr)\n</code></pre></li> </ul>"},{"location":"gmm-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The GMM implementation uses MeTTa function definitions with NumPy bindings:</p> <ol> <li>Centering    Subtract means from data via broadcasting.  </li> <li>Time Complexity: O(n\u00b7k\u00b7d).  </li> <li> <p>Memory: O(n\u00b7k\u00b7d).</p> </li> <li> <p>Mahalanobis Term    Compute $(x_i-\\mu_k)^T \\Sigma_k^{-1}(x_i-\\mu_k)$ with <code>np.einsum</code>.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d^2).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Gaussian PDF    Evaluate the multivariate Gaussian density using the Mahalanobis term, determinant, and normalization.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d^2).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Log\u2011Likelihood    Sum log of weighted PDFs across data points.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>Initialization (<code>gmm.init</code>) </p> </li> <li>Weights: uniform $\\frac{1}{K}$.  </li> <li>Means: random points via <code>np.choose</code>.  </li> <li>Covariances: data covariance + small identity noise.  </li> <li>Time Complexity: O(n\u00b7d^2).  </li> <li> <p>Memory: O(k\u00b7d^2).</p> </li> <li> <p>E\u2011Step (<code>gmm.e-step</code>)    Compute responsibilities $r_{ik} = \\pi_k p_k(x_i) / \\sum_j \\pi_j p_j(x_i)$.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li> <p>Memory: O(n\u00b7k).</p> </li> <li> <p>M\u2011Step (<code>gmm.m-step</code>)    Update:</p> </li> <li>$N_k = \\sum_i r_{ik}$</li> <li>$\\pi_k = N_k / n$</li> <li>$\\mu_k = (1/N_k) \\sum_i r_{ik} x_i$</li> <li> <p>$\\Sigma_k = (1/N_k) \\sum_i r_{ik} (x_i-\\mu_k)(x_i-\\mu_k)^T$  </p> </li> <li> <p>Time Complexity: O(n\u00b7k\u00b7d^2).  </p> </li> <li> <p>Memory: O(k\u00b7d^2 + n\u00b7k).</p> </li> <li> <p>Recursive EM Loop    Repeat E\u2011 and M\u2011Steps up to <code>max_iter</code>.  </p> </li> <li>Time Complexity: O(max_iter \u00b7 n \u00b7 k \u00b7 d^2).  </li> <li> <p>Memory: Tail\u2011calls reuse frames; peak O(n\u00b7k\u00b7d^2).</p> </li> <li> <p>Predict (<code>gmm.predict</code>)    Assign cluster by highest responsibility.  </p> </li> <li>Time Complexity: O(n\u00b7k).  </li> <li>Memory: O(n\u00b7k).</li> </ol>"},{"location":"gmm-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn provides <code>GaussianMixture</code> with C-optimized loops and multiple covariance options. Future work may include:</p> Feature MeTTa Implementation scikit\u2011learn Covariance types full only full, tied, diag, spherical Initialization random k\u2011means++, random Convergence control tol on log\u2011likelihood tol on log\u2011likelihood Parallelization single\u2011threaded single\u2011threaded"},{"location":"gmm-technical/#benchmark-setup","title":"Benchmark Setup","text":"Dataset MeTTa Time (s) scikit-learn Time (s) Silhouette Calinski-Harabasz Davies-Bouldin Adjusted Rand Index Normalized Mutual Info Adjusted Mutual Info varied 15.125 &lt;0.01 0.58807 1198.59 0.68460 0.94682 0.91605 0.91574 blobs 4.519 &lt;0.01 0.65379 1422.54 0.47993 0.96444 0.94787 0.94768 noisy_circles 149.801 &lt;0.01 0.34909 281.92 1.19307 -0.00199 0.00001 -0.00144 noisy_moon 78.848 &lt;0.01 0.35369 297.61 0.88993 0.23313 0.33222 0.33114 no-structure 151.017 &lt;0.01 0.22208 181.17 1.60891 0 0 0"},{"location":"gmm-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul:cluster:gmm)\n\n(let $params (gmm.fit X 3 50)\n  (println! (gmm.predict X $params))\n)\n</code></pre>"},{"location":"gmm-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Support additional covariance structures.</li> </ul>"},{"location":"gmm-technical/#conclusion","title":"Conclusion","text":"<p>The GMM implementation demonstrates MeTTa\u2019s ability to express probabilistic EM algorithms declaratively. While performance depends on NumPy backends, the clear, recursive definitions simplify future extensions.</p>"},{"location":"gmm-technical/#references","title":"References","text":"<ol> <li>Dempster, Laird &amp; Rubin (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm.  </li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.  </li> <li>MeTTa Language Specification. http://www.metta-lang.dev/spec</li> </ol>"},{"location":"kmeans-api-reference/","title":"metta_ul:cluster:kmeans","text":""},{"location":"kmeans-api-reference/#overview","title":"Overview","text":"<p>This module implements the K-Means clustering algorithm in MeTTa. K-Means is a popular unsupervised learning method for partitioning a dataset into <code>k</code> clusters based on feature similarity.</p>"},{"location":"kmeans-api-reference/#functions","title":"Functions","text":""},{"location":"kmeans-api-reference/#kmeansupdate","title":"<code>kmeans.update</code>","text":""},{"location":"kmeans-api-reference/#description","title":"Description:","text":"<p>This function updates the centroids based on the current cluster assignments. It computes the new centroids as the mean of all data points assigned to each cluster.</p>"},{"location":"kmeans-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$assignments</code>: One-hot encoded cluster assignment matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns","title":"Returns:","text":"<ul> <li>Updated cluster centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansassign","title":"<code>kmeans.assign</code>","text":""},{"location":"kmeans-api-reference/#description_1","title":"Description:","text":"<p>Assigns each data point to the nearest centroid.</p>"},{"location":"kmeans-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$centroids</code>: Current centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A one-hot encoded assignment matrix indicating which cluster each point belongs to.<ul> <li>Type: <code>(NPArray ($k $n))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansrecursion","title":"<code>kmeans.recursion</code>","text":""},{"location":"kmeans-api-reference/#description_2","title":"Description:","text":"<p>Recursively updates the centroids until the maximum number of iterations is reached.</p>"},{"location":"kmeans-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $d))</code></li> </ul> </li> <li><code>$centroids</code>: Initial centroids.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> <li><code>$max-iter</code>: Maximum number of iterations.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>Final cluster centroids after convergence or reaching the iteration limit.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeansfit","title":"<code>kmeans.fit</code>","text":""},{"location":"kmeans-api-reference/#description_3","title":"Description:","text":"<p>Main function to perform K-Means clustering. It initializes centroids randomly and iteratively updates them.</p>"},{"location":"kmeans-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> <li><code>$k</code>: Number of clusters.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$max-iter</code> (optional, default: 100): Maximum number of iterations.<ul> <li>Type: <code>Number</code></li> </ul> </li> <li><code>$tol</code> (optional, default: 0.0001): Tolerance on the shift of the centroids.<ul> <li>Type: <code>Number</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>Final cluster centroids after completion of the algorithm.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#kmeanspredict","title":"<code>kmeans.predict</code>","text":""},{"location":"kmeans-api-reference/#description_4","title":"Description:","text":"<p>Main function for predicting the labels of some test data after learning the centroids from the training data. It computes the assignments and then return the positions of the maximum assignments as the labels.</p>"},{"location":"kmeans-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$X</code>: Data points as a matrix.<ul> <li>Type: <code>(NPArray ($n $k))</code></li> </ul> </li> <li><code>$centroids</code>: The position of centroids of the clusters.<ul> <li>Type: <code>(NPArray ($k $d))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>predicted labels.<ul> <li>Type: <code>(NPArray ($n))</code></li> </ul> </li> </ul>"},{"location":"kmeans-api-reference/#usage","title":"Usage","text":"<p>To cluster a dataset <code>S</code> of type <code>(NPArray (n, d))</code> into 3 clusters with default settings: <pre><code>(=\n    (centroids)\n    (kmeans.fit (S) 3)\n)\n</code></pre> To specify a maximum number of 50 iterations and a tolerance of 0.001: <pre><code>(kmeans (S) 3 50 0.001)\n</code></pre> To predict the labels of a dataset <code>X</code> of type <code>(NPArray (m, d))</code> using <code>centroids</code>: <pre><code>(=\n    (assignments)\n    (kmeans.predict (X) (centroids))\n)\n</code></pre></p>"},{"location":"kmeans-api-reference/#notes","title":"Notes","text":"<ul> <li>The initial centroids are chosen randomly from the data points.</li> <li>The algorithm stops after reaching the iteration limit, but convergence is not guaranteed.</li> <li>The function <code>np.one_hot</code> is used to convert the assignment indices into a one-hot encoded matrix.</li> </ul>"},{"location":"kmeans-api-reference/#dependencies","title":"Dependencies","text":"<p>This module uses NumPy functions within MeTTa: - <code>np.div</code>: Element-wise division. - <code>np.matmul</code>: Matrix multiplication. - <code>np.sum</code>: Summation along a specified axis. - <code>np.linalg.norm</code>: Computes the L2 norm. - <code>np.sub</code>: Element-wise subtraction. - <code>np.expand_dims</code>: Adds dimensions to an array. - <code>np.choose</code>: Random selection of initial centroids. - <code>np.one_hot</code>: Converts indices to one-hot encoding. - <code>np.argmin</code>: Finds the index of the minimum value along an axis.</p>"},{"location":"kmeans-technical/","title":"KMeans Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 19, 2025 Version: 1.0  </p>"},{"location":"kmeans-technical/#abstract","title":"Abstract","text":"<p>This report describes an open\u2011source implementation of the KMeans clustering algorithm in the MeTTa language. It demonstrates MeTTa\u2019s meta\u00adgraph and functional capabilities by expressing key operations\u2014assignment, update, and recursion\u2014without native loop constructs. Benchmarks on six synthetic datasets show MeTTa\u2019s runtime (~3.8\u202fs) compared to scikit\u2011learn\u2019s single\u2011threaded C backend (&lt;\u202f0.009\u202fs), highlighting trade\u2011offs between expressiveness and performance.</p>"},{"location":"kmeans-technical/#introduction","title":"Introduction","text":"<p>Clustering partitions data into groups of similar points. KMeans is a widely used algorithm that minimizes within\u2011cluster variance. Embedding clustering in MeTTa:</p> <ul> <li>Expands MeTTa\u2019s numeric and declarative expressiveness.  </li> <li>Serves as a reference for future ML algorithm implementations.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa: a multi\u2011paradigm language for declarative and functional computations over knowledge (meta)graphs. See http://www.metta-lang.dev for details.</p>"},{"location":"kmeans-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>KMeans seeks to minimize <pre><code>J = \\sum_{i=1}^{k} \\sum_{x\\in C_i} \\|x - \\mu_i\\|^2\n</code></pre></p> <p>where $\\mu_i$ are centroids. We use Euclidean distance and continue untill either a fixed maximum of recursive iterations is reached or the change in cluster centers is less than some value in MeTTa. Recursion replaces loops due to MeTTa\u2019s execution model.</p>"},{"location":"kmeans-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa KMeans implementation uses a purely declarative, recursive style to express the core steps of the algorithm without explicit loops. Data is represented as NumPy arrays wrapped in MeTTa types, and centroids, assignments, and models are built via pattern\u2010matched function definitions.</p> <ol> <li>Initialization    A fixed number of <code>k</code> centroids are sampled at random from the input matrix <code>X</code> using MeTTa\u2019s binding to NumPy\u2019s array\u2010indexing (<code>np.choose</code>).  </li> <li>Time Complexity: O(k) to sample indices.  </li> <li> <p>Memory Overhead: O(k\u00b7d) to store the initial centroids.</p> </li> <li> <p>Assignment Step    For each of the <code>n</code> data points, Euclidean distances to all <code>k</code> centroids are computed via a broadcasted norm operation. The index of the nearest centroid is converted into a one\u2010hot encoded assignment vector.  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for distance calculations and argmin.  </li> <li> <p>Memory Overhead: O(n\u00b7k) for the one\u2010hot assignment matrix plus O(n\u00b7d) for intermediate expanded arrays.</p> </li> <li> <p>Update Step    Centroids are recomputed by multiplying the assignment matrix by the data matrix (<code>assignments \u00d7 X</code>) and normalizing by the cluster counts (sum over assignments).  </p> </li> <li>Time Complexity: O(n\u00b7k\u00b7d) for matrix multiplication and O(k\u00b7n) for sum reduction.  </li> <li> <p>Memory Overhead: O(k\u00b7d) for new centroids and O(k\u00b7n) for assignment reuse.</p> </li> <li> <p>Recursive Loop with Early Stopping    The functions <code>assign</code> and <code>update</code> are composed in a recursion. Each recursive call passes the newly updated centroids to the next iteration.  </p> </li> <li>Time Complexity (worst-case): O(max_iter \u00b7 n \u00b7 k \u00b7 d).  </li> <li>Time Complexity (average-case): O(m \u00b7 n \u00b7 k \u00b7 d), where m is the iteration count until tolerance is reached (m \u2264 max_iter).  </li> <li> <p>Memory Overhead: O(n\u00b7k + k\u00b7d + n\u00b7d); recursion can use tail\u2011call optimization, so stack does not grow.</p> </li> <li> <p>Model Fit &amp; Predict </p> </li> <li><code>kmeans.fit(X, k, max_iter, tol)</code> performs recursion and returns centroids.  </li> <li><code>kmeans.predict(X, centers)</code> uses <code>argmax</code> on assignment matrix to output labels.  </li> <li>Predict Complexity: O(n\u00b7k\u00b7d) for assignment plus O(n\u00b7k) for argmax.</li> </ol> <p>The early\u2011stop mechanism often reduces iterations needed, improving average\u2011case performance while preserving worst\u2011case bounds.</p>"},{"location":"kmeans-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>Both implementations are single\u2011threaded. scikit\u2011learn\u2019s KMeans uses optimized C loops and k\u2011means++ initialization by default.</p> Feature MeTTa scikit\u2011learn Initialization random via <code>np.choose</code> k\u2011means++ Distance metric Euclidean Euclidean Control flow recursion loops (C backend) Parallelization single\u2011threaded single\u2011threaded Convergence criterion tol on centroid shift tol on centroid shift"},{"location":"kmeans-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each; seed=30 unless noted): <code>blobs</code>, <code>noisy_moons</code>, <code>noisy_circles</code>, <code>no_structure</code>, <code>aniso</code>, <code>varied</code>.  </li> <li>Synthetic generation uses scikit\u2011learn utilities (not shown).  </li> <li>Environment: CPU = 4\u202fcores @ 3.0\u202fGHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1.</li> </ul>"},{"location":"kmeans-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset MeTTa Time (s) scikit-learn Time (s) Silhouette Calinski-Harabasz Davies-Bouldin Adjusted Rand Index Normalized Mutual Info Adjusted Mutual Info varied 0.464 &lt;0.01 0.63991 1549.90 0.61256 0.74010 0.74164 0.74068 blobs 0.239 &lt;0.01 0.65424 1424.91 0.47929 0.97027 0.95445 0.95428 noisy_circles 0.797 &lt;0.01 0.35274 287.33 1.18125 -0.00186 0.00010 -0.00135 noisy_moon 0.426 &lt;0.01 0.49557 690.81 0.81197 0.48338 0.38565 0.38476 no-structure 0.986 &lt;0.01 0.38135 388.93 0.86894 0 0 0 <p>MeTTa\u2019s recursive overhead yields ~3.8\u202fs runtimes, whereas scikit\u2011learn completes in under 0.01\u202fs. Despite this gap, MeTTa\u2019s implementation showcases the language\u2019s ability to express complex numerical algorithms.</p>"},{"location":"kmeans-technical/#usage-example","title":"Usage Example","text":"<p><pre><code>(import! &amp;self metta_ul)\n\n(let $clusters (kmeans.fit X 3)\n  (println! $clusters)\n</code></pre> This snippet: 1. Imports the clustering module. 2. Fits KMeans with <code>k=3</code>. 3. Prints cluster labels.</p>"},{"location":"kmeans-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Initialization: add k\u2011means++ seeding to improve convergence.</li> </ul>"},{"location":"kmeans-technical/#conclusion","title":"Conclusion","text":"<p>This implementation verifies that MeTTa can express core ML algorithms declaratively. While performance lags optimized C libraries, the clarity and extensibility in MeTTa pave the way for further ML primitives in the language.</p>"},{"location":"kmeans-technical/#references","title":"References","text":"<ol> <li>MacQueen, J. (1967). Some Methods for Classification and Analysis of Multivariate Observations.  </li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.  </li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"spectral-api-reference/","title":"metta_ul:cluster:spectral-clustering","text":""},{"location":"spectral-api-reference/#overview","title":"Overview","text":"<p>This module implements the Spectral Clustering algorithm. It supports two methods for constructing the affinity matrix: RBF (Radial Basis Function) kernel and KNN binary graph. The algorithm computes the normalized graph Laplacian, performs eigen-decomposition to extract spectral embeddings, normalizes these embeddings, and clusters them using k-means. The algorithm is designed to cluster data that may not be linearly separable, leveraging the eigenstructure of the Laplacian.</p>"},{"location":"spectral-api-reference/#function-definitions","title":"Function Definitions","text":""},{"location":"spectral-api-reference/#spectral-clusteringsquare-norm","title":"<code>spectral-clustering.square-norm</code>","text":"<p>Computes the square norm for each row (data point) in the dataset.</p>"},{"location":"spectral-api-reference/#parameters","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns","title":"Returns:","text":"<ul> <li>A vector where each element is the sum of squares of the components of the corresponding data point.</li> <li>Type: <code>(NPArray ($N 1)</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringsquare-distance-matrix","title":"<code>spectral-clustering.square-distance-matrix</code>","text":"<p>Computes the matrix of squared Euclidean distances between each pair of data points in the dataset.</p>"},{"location":"spectral-api-reference/#parameters_1","title":"Parameters:","text":"<ul> <li><code>$square-norm-X</code>: The vector of square norms computed from <code>$X</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> <li><code>$X</code>: The dataset.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_1","title":"Returns:","text":"<ul> <li>A square matrix where the entry at (i, j) is the squared Euclidean distance between data point i and j.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringrbf-affinity-matrix","title":"<code>spectral-clustering.rbf-affinity-matrix</code>","text":"<p>Generates the RBF (Radial Basis Function) affinity matrix from a squared distance matrix.</p>"},{"location":"spectral-api-reference/#parameters_2","title":"Parameters:","text":"<ul> <li><code>$sqr-distance-matrix-X</code>: The matrix of squared distances between data points.</li> <li>Type: <code>(NPArray ($N $N))</code></li> <li><code>$rbf-kernel-sigma</code>: The sigma parameter for the RBF kernel.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_2","title":"Returns:","text":"<ul> <li>A symmetric affinity matrix where each entry represents the similarity between a pair of data points based on the RBF kernel.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringcompute-rbf-affinity-matrix","title":"<code>spectral-clustering.compute-rbf-affinity-matrix</code>","text":"<p>Computes the complete RBF affinity matrix from the input dataset by combining square norm computation, distance matrix calculation, and RBF kernel application.</p>"},{"location":"spectral-api-reference/#parameters_3","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$rbf-kernel-sigma</code>: The sigma parameter for the RBF kernel.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_3","title":"Returns:","text":"<ul> <li>A symmetric RBF affinity matrix.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringcompute-knn-binary-graph-affinity-matrix","title":"<code>spectral-clustering.compute-knn-binary-graph-affinity-matrix</code>","text":"<p>Computes a binary affinity matrix based on k-nearest neighbors (KNN). Each data point is connected to its k nearest neighbors with weight 1, and all other connections have weight 0.</p>"},{"location":"spectral-api-reference/#parameters_4","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$n-neighbors</code>: The number of nearest neighbors to connect to each data point.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_4","title":"Returns:","text":"<ul> <li>A binary affinity matrix where 1 indicates a connection between k-nearest neighbors and 0 otherwise.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringdegree","title":"<code>spectral-clustering.degree</code>","text":"<p>Computes the degree of each node (data point) based on the affinity matrix.</p>"},{"location":"spectral-api-reference/#parameters_5","title":"Parameters:","text":"<ul> <li><code>$W</code>: The affinity matrix.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_5","title":"Returns:","text":"<ul> <li>A vector where each element is the sum of the corresponding row in <code>$W</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringinverse-degree-matrix","title":"<code>spectral-clustering.inverse-degree-matrix</code>","text":"<p>Constructs the inverse degree matrix used for normalization.</p>"},{"location":"spectral-api-reference/#parameters_6","title":"Parameters:","text":"<ul> <li><code>$degree-W</code>: The vector of node degrees computed from <code>$W</code>.</li> <li>Type: <code>(NPArray ($N 1))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_6","title":"Returns:","text":"<ul> <li>A diagonal matrix where each diagonal element is the inverse square root of the corresponding degree.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringnormalized-laplacian","title":"<code>spectral-clustering.normalized-laplacian</code>","text":"<p>Computes the normalized graph Laplacian from the affinity matrix and its inverse degree matrix.</p>"},{"location":"spectral-api-reference/#parameters_7","title":"Parameters:","text":"<ul> <li><code>$W</code>: The affinity matrix.</li> <li>Type: <code>(NPArray ($N $N))</code></li> <li><code>$inverse-degree-matrix-W</code>: The inverse degree matrix computed from <code>$W</code>.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_7","title":"Returns:","text":"<ul> <li>The normalized Laplacian matrix defined as I - D^{-1/2} W D^{-1/2}, where I is the identity matrix.</li> <li>Type: (NPArray ($N $N))</li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigh","title":"<code>spectral-clustering.eigh</code>","text":"<p>Performs eigen-decomposition on a given matrix.</p>"},{"location":"spectral-api-reference/#parameters_8","title":"Parameters:","text":"<ul> <li><code>$X</code>: The matrix to decompose (typically the normalized Laplacian).</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_8","title":"Returns:","text":"<ul> <li>A tuple containing the eigenvalues and eigenvectors of <code>$X</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigenvalues","title":"<code>spectral-clustering.eigenvalues</code>","text":"<p>Extracts the eigenvalues from the result of the eigen-decomposition.</p>"},{"location":"spectral-api-reference/#parameters_9","title":"Parameters:","text":"<ul> <li><code>$eigh-X</code>: The tuple returned from <code>spectral-clustering.eigh</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#returns_9","title":"Returns:","text":"<ul> <li>A vector containing the eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigenvectors","title":"<code>spectral-clustering.eigenvectors</code>","text":"<p>Extracts the eigenvectors from the result of the eigen-decomposition.</p>"},{"location":"spectral-api-reference/#parameters_10","title":"Parameters:","text":"<ul> <li><code>$eigh-X</code>: The tuple returned from <code>spectral-clustering.eigh</code>.</li> <li>Type: <code>EighResult</code></li> </ul>"},{"location":"spectral-api-reference/#returns_10","title":"Returns:","text":"<ul> <li>A matrix whose columns correspond to the eigenvectors of <code>$X</code>.</li> <li>Type: <code>(NPArray ($N $N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringeigval-top-k-index","title":"<code>spectral-clustering.eigval-top-k-index</code>","text":"<p>Finds the indices corresponding to the smallest k eigenvalues (after sorting).</p>"},{"location":"spectral-api-reference/#parameters_11","title":"Parameters:","text":"<ul> <li><code>$eigval-L</code>: The vector of eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> <li><code>$k</code>: The number of top eigenvalue indices to select.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_11","title":"Returns:","text":"<ul> <li>A vector of indices for the top k eigenvalues.</li> <li>Type: <code>(NPArray ($N))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringspectral-embeddings","title":"<code>spectral-clustering.spectral-embeddings</code>","text":"<p>Computes the spectral embeddings by selecting the top k eigenvectors based on their eigenvalues.</p>"},{"location":"spectral-api-reference/#parameters_12","title":"Parameters:","text":"<ul> <li><code>$eigh-I</code>: The eigen-decomposition result of the normalized Laplacian.</li> <li>Type: <code>EighResult</code></li> <li><code>$k</code>: The number of clusters (and hence dimensions for the embeddings).</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_12","title":"Returns:","text":"<ul> <li>A matrix of spectral embeddings extracted from the selected eigenvectors.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringrow-normalize","title":"<code>spectral-clustering.row-normalize</code>","text":"<p>Normalizes each row of a matrix to have unit norm.</p>"},{"location":"spectral-api-reference/#parameters_13","title":"Parameters:","text":"<ul> <li><code>$X</code>: A matrix (such as the spectral embeddings).</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#returns_13","title":"Returns:","text":"<ul> <li>The row-normalized version of <code>$X</code>.</li> <li>Type: <code>(NPArray ($N $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringcluster","title":"<code>spectral-clustering.cluster</code>","text":"<p>Clusters the spectral embeddings using the k-means algorithm.</p>"},{"location":"spectral-api-reference/#parameters_14","title":"Parameters:","text":"<ul> <li><code>$X</code>: The spectral embeddings matrix.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_14","title":"Returns:","text":"<ul> <li>The centroids obtained after clustering the row-normalized spectral embeddings with k-means.</li> <li>Type: <code>(NPArray ($K $D))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringfit-full-version","title":"<code>spectral-clustering.fit</code> (Full Version)","text":"<p>Performs the complete spectral clustering process on the dataset with configurable affinity computation method.</p>"},{"location":"spectral-api-reference/#parameters_15","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> <li><code>$affinity-mode</code>: The method for computing the affinity matrix. Supported values:</li> <li><code>\"rbf-affinity-matrix\"</code>: Uses RBF kernel for similarity computation</li> <li><code>\"binary-knn-graph\"</code>: Uses k-nearest neighbors binary graph</li> <li>Type: <code>String</code></li> <li><code>$affinity-param</code>: Parameter for the chosen affinity method:</li> <li>For RBF: the sigma parameter for the kernel</li> <li>For KNN: the number of nearest neighbors</li> <li>Type: <code>Number</code></li> <li><code>$max-kmeans-iter</code>: The maximum number of iterations for the k-means algorithm.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_15","title":"Returns:","text":"<ul> <li>A tuple containing spectral embeddings and the final centroids computed from clustering the spectral embeddings.</li> <li>Type: <code>((NPArray ($N $C)) (NPArray ($K $C)))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringfit-simplified-version","title":"<code>spectral-clustering.fit</code> (Simplified Version)","text":"<p>Performs spectral clustering with default parameters (RBF affinity with sigma=0.1 and 10 k-means iterations).</p>"},{"location":"spectral-api-reference/#parameters_16","title":"Parameters:","text":"<ul> <li><code>$X</code>: The dataset, represented as an array of data points.</li> <li>Type: <code>(NPArray ($N $D))</code></li> <li><code>$num-clusters</code>: The desired number of clusters.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_16","title":"Returns:","text":"<ul> <li>A tuple containing spectral embeddings and the final centroids computed from clustering the spectral embeddings.</li> <li>Type: <code>((NPArray ($N $C)) (NPArray ($K $C)))</code></li> </ul>"},{"location":"spectral-api-reference/#spectral-clusteringpredict","title":"<code>spectral-clustering.predict</code>","text":"<p>Predicts the cluster assignment for each data point in the dataset using the spectral embeddings and computed centroids.</p>"},{"location":"spectral-api-reference/#parameters_17","title":"Parameters:","text":"<ul> <li>A tuple <code>($embeddings $centroids)</code>, where:</li> <li><code>$embeddings</code>: The spectral embedding matrix.</li> <li><code>$centroids</code>: The centroids obtained from the clustering step.</li> <li>Type: <code>((NPArray ($N $C)) (NPArray ($K $C)))</code></li> <li><code>$num-clusters</code>: The number of clusters.</li> <li>Type: <code>Number</code></li> </ul>"},{"location":"spectral-api-reference/#returns_17","title":"Returns:","text":"<ul> <li>A vector of cluster labels, one for each data point, determined by assigning each point to the nearest centroid.</li> <li>Type: <code>((NPArray ($N)))</code></li> </ul>"},{"location":"spectral-api-reference/#usage","title":"Usage","text":""},{"location":"spectral-api-reference/#using-rbf-affinity-matrix","title":"Using RBF Affinity Matrix","text":"<p>To perform spectral clustering on a dataset <code>S</code> with RBF kernel: <pre><code>(=\n    (embeddings-and-centroids)\n    (spectral-clustering.fit S 3 \"rbf-affinity-matrix\" 0.1 100)\n)\n</code></pre></p>"},{"location":"spectral-api-reference/#using-knn-binary-graph","title":"Using KNN Binary Graph","text":"<p>To perform spectral clustering on a dataset <code>S</code> with KNN binary graph (connecting each point to its 5 nearest neighbors): <pre><code>(=\n    (embeddings-and-centroids)\n    (spectral-clustering.fit S 3 \"binary-knn-graph\" 5 100)\n)\n</code></pre></p>"},{"location":"spectral-api-reference/#using-default-parameters","title":"Using Default Parameters","text":"<p>To predict the cluster labels for the dataset <code>S</code>: <pre><code>(=\n    (cluster-labels)\n    (spectral-clustering.predict\n        (embeddings-and-centroids)        \n        3\n    )    \n)\n</code></pre></p>"},{"location":"spectral-method/","title":"Spectral Clustering","text":"<p>https://en.wikipedia.org/wiki/Spectral_clustering</p> <p>The spectral clustering algorithm reduces the dimensionality of the input data by leveraging  the eigenvalues and eigenvectors derived from the data's similarity matrix, and then applies  standard clustering methods, such as k-means, on the reduced data to identify clusters.</p> <p>Specifically, the algorithm constructs a graph structure using the similarity matrix of the data set,  where each entry represents the similarity score between the corresponding pair of data points.  It then employs the eigenvectors associated to the first k eigenvalues of the  normalized graph Laplacian as input features for the k-means algorithm.</p> <p>The data set is represented by the matrix X with shape [N, D], where N is the number of  data points and D is the dimension of the input space. Our goal is to partition the data points into K clusters. Below is a clear, step-by-step outline of the basic spectral clustering algorithm: 1. Compute the similarity matrix W given the input data X. The similarity matrix W is a symmetric matrix with the shape [N, N], where W[i, j] stores the similarity between data points X[i] and X[j]. A Gaussian (RBF) kernel is used for computing the similarity of data points X[i] and X[j], i.e.,  W[i, j] = exp(-|| X[i] - X[j] ||^2 / (2 * sigma^2)), where sigma is the scale parameter of the  Gaussian kernel. 2. Using the similarity matrix, compute the normalized graph Laplacian L_norm: L_norm = I - D^(-1/2) W D^(-1/2), where I is the identity matrix of shape [N, N], and D is the  degree matrix with D[i, i] = \\sum_j W[i, j]. 3. Compute the spectral embeddings U as the eigenvectors of the k smallest eigenvalues of L. The spectral embeddings matrix U has the shape [N, K].  4. Normalize the rows of U to have the sum equal to one. 5. Apply the k-means clustering algorithm to the spectral embeddings matrix U to find the K clusters.</p>"},{"location":"spectral-technical/","title":"Spectral Clustering in MeTTa","text":"<p>Author: Ramin Barati, Amirhossein Nouranizadeh, Farhoud Mojahedzadeh Date: May 24, 2025 Version: 1.0  </p>"},{"location":"spectral-technical/#abstract","title":"Abstract","text":"<p>This report presents an open\u2011source implementation of spectral clustering in the MeTTa language. By leveraging MeTTa's declarative approach and NumPy bindings, we express graph\u2011based clustering via eigendecomposition of the Laplacian matrix. Our implementation constructs similarity graphs with RBF kernels, computes normalized Laplacians, and extracts low\u2011dimensional embeddings for k\u2011means partitioning. Benchmarks show competitive clustering quality with runtimes of ~1.1s versus scikit\u2011learn's ~0.05s, demonstrating MeTTa's capability to express complex mathematical algorithms while highlighting areas for optimization.</p>"},{"location":"spectral-technical/#introduction","title":"Introduction","text":"<p>Spectral clustering transforms data clustering into a graph partitioning problem, offering advantages over traditional methods for complex, non\u2011convex cluster shapes. Implementing spectral clustering in MeTTa:</p> <ul> <li>Demonstrates MeTTa's capability for linear algebra and eigendecomposition.  </li> <li>Extends MeTTa's ML toolkit with a graph\u2011based clustering method.  </li> <li>Provides a foundation for future spectral methods in MeTTa.  </li> <li>Targets open\u2011source users in AI and Data Science.</li> </ul> <p>MeTTa is a multi\u2011paradigm language for declarative and functional computations over knowledge metagraphs. See http://www.metta\u2011lang.dev for details.</p>"},{"location":"spectral-technical/#algorithm-overview","title":"Algorithm Overview","text":"<p>Spectral clustering operates on the principle that cluster structure can be revealed through the spectrum (eigenvalues and eigenvectors) of a graph Laplacian derived from data similarities.</p> <p>Key steps:</p> <ol> <li> <p>Similarity Graph Construction: Create an affinity matrix $W$ using a Gaussian kernel    <pre><code>W_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n</code></pre></p> </li> <li> <p>Laplacian Matrix: Compute the normalized Laplacian $L_{sym}$    <pre><code>L_{sym} = I - D^{-1/2}WD^{-1/2}\n</code></pre>    where $D$ is the diagonal degree matrix with $D_{ii} = \\sum_j W_{ij}$</p> </li> <li> <p>Spectral Embedding: Find the $k$ eigenvectors corresponding to the smallest eigenvalues of $L_{sym}$, forming a low\u2011dimensional embedding space.</p> </li> <li> <p>Clustering: Apply k\u2011means to the normalized rows of the eigenvector matrix.</p> </li> </ol> <p>The algorithm exploits spectral graph theory: the multiplicity of the eigenvalue 0 equals the number of connected components, and the corresponding eigenvectors encode component membership.</p>"},{"location":"spectral-technical/#metta-implementation-details","title":"MeTTa Implementation Details","text":"<p>The MeTTa implementation utilizes NumPy bindings for matrix operations and eigendecomposition, with clear separation of algorithmic steps:</p> <ol> <li>Affinity Matrix Construction    First computes squared norms and pairwise distances, then applies RBF kernel:    <pre><code>(spectral-clustering.rbf-affinity-matrix $sqr-distance-matrix-X $rbf-kernel-sigma)\n</code></pre></li> <li>Time Complexity: O(n\u00b2\u00b7d) for distance matrix, O(n\u00b2) for kernel application.  </li> <li> <p>Space Complexity: O(n\u00b2) for the affinity matrix.</p> </li> <li> <p>Degree Matrix and Laplacian    Computes node degrees by summing affinities, then builds the normalized Laplacian:    <pre><code>(spectral-clustering.normalized-laplacian $W $inverse-degree-matrix-W)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b2) for degree calculation, O(n\u00b3) for Laplacian normalization with matrix multiplication.  </li> <li> <p>Space Complexity: O(n\u00b2) for the Laplacian.</p> </li> <li> <p>Eigendecomposition    Extracts eigenvalues and eigenvectors of the Laplacian:    <pre><code>(spectral-clustering.eigh $X)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b3) for full eigendecomposition.  </li> <li> <p>Space Complexity: O(n\u00b2) for eigenvectors.</p> </li> <li> <p>Spectral Embedding    Selects and normalizes eigenvectors for the k smallest eigenvalues:    <pre><code>(spectral-clustering.row-normalize \n  (spectral-clustering.spectral-embeddings $eigh-I $k))\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k) for selection and normalization.  </li> <li> <p>Space Complexity: O(n\u00b7k) for the embedding.</p> </li> <li> <p>Clustering    Applies k\u2011means to the normalized embeddings:    <pre><code>(kmeans.fit $embeddings $num-clusters $max-kmeans-iter 0.0001)\n</code></pre></p> </li> <li>Time Complexity: O(t\u00b7n\u00b7k\u00b2) where t is iterations of k\u2011means.  </li> <li> <p>Space Complexity: O(n\u00b7k) for assignments.</p> </li> <li> <p>Pipeline Integration    The complete pipeline combines these steps in a nested let\u2011expression structure:    <pre><code>(spectral-clustering.fit $X $num-clusters $rbf-kernel-sigma $max-kmeans-iter)\n</code></pre></p> </li> <li>Overall Time Complexity: O(n\u00b3) dominated by eigendecomposition.  </li> <li> <p>Overall Space Complexity: O(n\u00b2) dominated by the affinity and Laplacian matrices.</p> </li> <li> <p>Prediction    Assigns new data to clusters based on nearest centroids:    <pre><code>(spectral-clustering.predict ($embeddings $centroids) $num-clusters)\n</code></pre></p> </li> <li>Time Complexity: O(n\u00b7k) for assignment calculation.  </li> <li>Space Complexity: O(n\u00b7k) for assignment matrix.</li> </ol> <p>The implementation handles the key algorithm steps declaratively, with eigendecomposition being the computational bottleneck for large datasets.</p>"},{"location":"spectral-technical/#comparison-with-scikitlearn","title":"Comparison with scikit\u2011learn","text":"<p>scikit\u2011learn's <code>SpectralClustering</code> offers more options and optimizations for larger datasets:</p> Feature MeTTa Implementation scikit\u2011learn Affinity methods RBF kernel only RBF, nearest neighbors, precomputed Laplacian types normalized symmetric normalized, unnormalized, random walk Eigensolvers full decomposition via NumPy ARPACK, LOBPCG, AMG (sparse options) Scaling techniques none Nystr\u00f6m method for large datasets Final clustering k\u2011means only k\u2011means or discretization Implementation declarative with NumPy bindings optimized C/Cython with specialized solvers <p>The MeTTa implementation provides core functionality while scikit\u2011learn offers additional options for scalability and customization.</p>"},{"location":"spectral-technical/#benchmark-setup","title":"Benchmark Setup","text":"<ul> <li>Datasets (500 samples each, synthetic generation via scikit\u2011learn):</li> <li><code>noisy_moons</code>: Two interleaving half\u2011circles</li> <li><code>varied</code>: Varied variance blobs</li> <li><code>no_structure</code>: Random noise</li> <li>Environment: CPU = 4 cores @ 3.0 GHz, Hyperon 0.2.2, NumPy 2.2.2, scikit\u2011learn 1.6.1</li> </ul>"},{"location":"spectral-technical/#results","title":"Results","text":"<p>Performance comparison:</p> Dataset Time (s) Silhouette Calinski-Harabasz Davies-Bouldin ARI NMI AMI no-structure 0.69 0.370 359.72 0.863 0 0 0 blobs 1.57 0.457 268.15 0.594 0.568 0.729 0.728 noisy_circles 1.18 0.114 0.0077 240.59 1.000 1.000 1.000 noisy_moon 0.73 0.385 429.54 1.028 1.000 1.000 1.000 varied 0.76 0.627 1474.97 0.642 0.843 0.828 0.827"},{"location":"spectral-technical/#usage-example","title":"Usage Example","text":"<pre><code>(import! &amp;self metta_ul) \n(import! &amp;self spectral-clustering)\n\n(let \n    $labels \n    (spectral-clustering.predict $fit-outputs 2)\n    (println! $labels)\n)\n</code></pre> <p>This snippet: 1. Imports required modules 2. Fits spectral clustering with <code>k=2</code> clusters 3. Predicts cluster assignments 4. Prints the resulting labels</p>"},{"location":"spectral-technical/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ul> <li>Scalability: The O(n\u00b2) memory requirement for the affinity matrix and O(n\u00b3) time complexity for eigendecomposition limit scaling to large datasets.</li> <li>Affinity options: Currently only supports RBF kernel; could add nearest\u2011neighbor and custom affinities.</li> <li>Eigensolver efficiency: Could integrate specialized solvers like ARPACK for large sparse matrices.</li> <li>Hyperparameter selection: Automatic sigma estimation for the RBF kernel would improve usability.</li> <li>Alternative Laplacians: Add unnormalized and random walk variants.</li> </ul>"},{"location":"spectral-technical/#conclusion","title":"Conclusion","text":"<p>The MeTTa implementation of spectral clustering demonstrates the language's capability to express complex numerical algorithms declaratively. Despite performance differences compared to optimized C/Cython code, the implementation achieves excellent clustering quality on non\u2011convex datasets. The clear functional decomposition allows for future extensions and optimizations while serving as a reference for spectral methods in MeTTa.</p>"},{"location":"spectral-technical/#references","title":"References","text":"<ol> <li>Ng, Jordan, Weiss (2002). On Spectral Clustering: Analysis and an Algorithm.</li> <li>von Luxburg (2007). A Tutorial on Spectral Clustering.</li> <li>Pedregosa et al. (2011). scikit\u2011learn: Machine Learning in Python.</li> <li>MeTTa Language Specification. http://www.metta\u2011lang.dev/spec</li> </ol>"},{"location":"visualization/","title":"Virtualization in <code>metta_ul</code>: Grounding Pandas, Matplotlib, and Dimensionality Reduction Libraries","text":""},{"location":"visualization/#overview","title":"Overview","text":"<p>The goal of <code>metta_ul</code> is to ground essential Python libraries inside MeTTa, enabling symbolic code to leverage powerful data science tools:</p> <ul> <li>Pandas for data manipulation</li> <li>Matplotlib for plotting</li> <li>Scikit-learn for machine learning, including dimensionality reduction</li> </ul> <p>We started by grounding core functions explicitly but later switched to a general import mechanism with <code>py-import!</code> and <code>py-from!</code> macros, allowing dynamic binding of Python modules and their functions.</p>"},{"location":"visualization/#grounding-approach","title":"Grounding Approach","text":""},{"location":"visualization/#explicit-grounding-vs-general-import-macros","title":"Explicit Grounding vs General Import Macros","text":"<ul> <li>Explicit Grounding: Ground individual functions or classes one by one (e.g., <code>pd.read_csv</code>, <code>plt.plot</code>).</li> <li>General Import Macros: Use <code>py-import!</code> and <code>py-from!</code> to import modules or functions dynamically, providing flexibility and scalability.</li> </ul>"},{"location":"visualization/#import-macros","title":"Import Macros","text":""},{"location":"visualization/#py-import","title":"<code>py-import!</code>","text":"<p>Import whole modules or assign aliases:</p> <pre><code>! (py-import! pandas as pd)\n! (py-import! matplotlib.pyplot as plt)\n! (py-import! sklearn.decomposition as decomposition)\n! (py-import! sklearn.manifold as manifold)\n</code></pre>"},{"location":"visualization/#py-from","title":"<code>py-from!</code>","text":"<p>Import specific functions or classes:</p> <pre><code>! (py-from! sklearn.decomposition import PCA)\n! (py-from! sklearn.manifold import TSNE)\n! (py-from! pandas import DataFrame)\n</code></pre>"},{"location":"visualization/#example-usage","title":"Example Usage","text":""},{"location":"visualization/#pandas-import-and-export-csv","title":"Pandas: Import and Export CSV","text":"<pre><code>! (py-import! pandas as pd)\n\n(= (load-csv $filename) (pd.read_csv $filename))\n(= (save-csv $df $filename) (pd.DataFrame.to_csv $df $filename))\n</code></pre>"},{"location":"visualization/#matplotlib-plotting-line-chart","title":"Matplotlib: Plotting Line Chart","text":"<pre><code>! (py-import! matplotlib.pyplot as plt)\n\n(= (plot-line $x $y)\n  (plt.plot $x $y)\n  (plt.show))\n</code></pre>"},{"location":"visualization/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Use scikit-learn's PCA and t-SNE for reducing data dimensionality:</p> <pre><code>! (py-from! sklearn.decomposition import PCA)\n! (py-from! sklearn.manifold import TSNE)\n! (py-import! numpy as np)\n\n(= (reduce-pca $data $components)\n  (let* (\n    ($pca (PCA n_components $components))\n    ($reduced (pca.fit_transform $data))\n  )\n  $reduced))\n\n(= (reduce-tsne $data $components)\n  (let* (\n    ($tsne (TSNE n_components $components))\n    ($reduced (tsne.fit_transform $data))\n  )\n  $reduced)\n)\n</code></pre>"},{"location":"visualization/#summary","title":"Summary","text":"<ul> <li>Virtualization in <code>metta_ul</code> grounds important Python libraries into MeTTa.</li> <li><code>py-import!</code> and <code>py-from!</code> macros dynamically bind Python modules/functions, enabling concise and extensible code.</li> <li>This allows integration of data science workflows \u2014 data loading (pandas), visualization (matplotlib), and machine learning (scikit-learn) \u2014 directly in MeTTa programs.</li> </ul>"}]}